{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "MIN_DATE = date(2025, 6, 10)  # Use simple date object (no time)\n",
    "BASE_URL = \"https://www.teamblind.com\"\n",
    "TOPIC_URL = f\"{BASE_URL}/topics/General-Topics/Layoffs\"\n",
    "OUTPUT_FILE = 'posttesting.json'  # JSON Lines format\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure Selenium\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,800\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Network.setUserAgentOverride\",\n",
    "        {\"userAgent\": HEADERS[\"User-Agent\"]}\n",
    "    )\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_ld(html):\n",
    "    \"\"\"Extract JSON-LD data from post page\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    script = soup.find('script', {'id': 'article-discussion-forum-posting-schema', 'type': 'application/ld+json'})\n",
    "    \n",
    "    if not script:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        return json.loads(script.string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_post_details(html_text):\n",
    "    \"\"\"\n",
    "    Extract post details from the HTML response text.\n",
    "    \n",
    "    Args:\n",
    "        html_text (str): The HTML content of the page as a string.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted details:\n",
    "              - like_count (int)\n",
    "              - view_count (int)\n",
    "              - author_company (str)\n",
    "              - author_id (str)\n",
    "              - button_container (bool) (True if found, False otherwise)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    \n",
    "    # Find the button container\n",
    "    button_container = soup.find('div', class_='flex gap-2 md:gap-4')\n",
    "    \n",
    "    # Initialize counts\n",
    "    like_count = 0\n",
    "    view_count = 0\n",
    "    if button_container:\n",
    "        # Find like button by aria-label\n",
    "        like_button = button_container.find('button', {'aria-label': 'Like this post'})\n",
    "        if like_button and 'data-count' in like_button.attrs:\n",
    "            try:\n",
    "                like_count = int(like_button['data-count'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Find view button by aria-label\n",
    "        view_button = button_container.find('button', {'aria-label': 'Views'})\n",
    "        if view_button and 'data-count' in view_button.attrs:\n",
    "            try:\n",
    "                view_count = int(view_button['data-count'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    # Extract author details\n",
    "    author_company = \"\"\n",
    "    author_id = \"\"\n",
    "    author_div = soup.find('div', class_='flex h-full items-center text-xs text-gray-800')\n",
    "    if author_div:\n",
    "        # Extract company name\n",
    "        company_link = author_div.find('a')\n",
    "        if company_link:\n",
    "            author_company = company_link.get_text(strip=True)\n",
    "        \n",
    "        # Extract author ID - find the last text node in the div\n",
    "        text_nodes = [text for text in author_div.stripped_strings]\n",
    "        if text_nodes:\n",
    "            # Author ID is the last text node after the SVG\n",
    "            author_id = text_nodes[-1]\n",
    "    \n",
    "    return {\n",
    "        \"like_count\": like_count,\n",
    "        \"view_count\": view_count,\n",
    "        \"author_company\": author_company,\n",
    "        \"author_id\": author_id,\n",
    "        \"button_container\": bool(button_container)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments(comments):\n",
    "    \"\"\"Recursively process comments and their replies\"\"\"\n",
    "    processed = []\n",
    "    for comment in comments:\n",
    "        try:\n",
    "            # Extract basic comment info\n",
    "            comment_data = {\n",
    "                \"text\": comment.get(\"text\", \"\"),\n",
    "                \"date\": comment.get(\"datePublished\", \"\")[:10],  # Only date part\n",
    "                \"author\": comment.get(\"author\", {}).get(\"name\", \"\") if \"author\" in comment else \"\",\n",
    "                \"upvoteCount\": comment.get(\"upvoteCount\", 0),\n",
    "                \"commentCount\": comment.get(\"commentCount\", 0),\n",
    "                \"replies\": []\n",
    "            }\n",
    "            \n",
    "            # Process nested replies if they exist\n",
    "            if \"comment\" in comment and comment[\"comment\"]:\n",
    "                comment_data[\"replies\"] = process_comments(comment[\"comment\"])\n",
    "            \n",
    "            processed.append(comment_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing comment: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return processed\n",
    "\n",
    "def scrape_post(url):\n",
    "    \"\"\"Scrape individual post page and return (data, is_old) tuple\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        post_data = extract_json_ld(response.text)\n",
    "        if not post_data:\n",
    "            print(f\"No JSON-LD data found for {url}\")\n",
    "            return None, False\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        if post_date < MIN_DATE: return None, True  # Post is too old\n",
    "\n",
    "        # Extract post details\n",
    "        post_details = extract_post_details(response.text)\n",
    "\n",
    "        # Extract and format comments\n",
    "        comments = []\n",
    "        if \"comment\" in post_data:\n",
    "            try:\n",
    "                comments = process_comments(post_data[\"comment\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing comments: {str(e)}\")\n",
    "        else:\n",
    "            print(\"No comments element found in post data\")\n",
    "\n",
    "        return {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_date_str,  # Store only the date part\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"author\": post_details[\"author_id\"], \n",
    "            \"authorCompany\": post_details[\"author_company\"],\n",
    "            \"likeCount\": post_details[\"like_count\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"],\n",
    "            \"viewCount\": post_details[\"view_count\"],\n",
    "            \"comments\": comments\n",
    "        }, False\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'headline': 'AI will result in net job loss',\n",
       "  'text': 'Thanks to asshole AI coders eventually humans will lose jobs to fking AI',\n",
       "  'date': '2025-06-17',\n",
       "  'url': 'https://www.teamblind.com/post/ai-will-result-in-net-job-loss-l37jl741',\n",
       "  'author': 'PNLi05',\n",
       "  'authorCompany': '',\n",
       "  'likeCount': 74,\n",
       "  'commentCount': 116,\n",
       "  'viewCount': 8085,\n",
       "  'comments': [{'text': 'Grandpa was a whaler and produced whale oil for lamps. Thanks to asshole oil companies he lost his job\\nUncle was a coal miner. Thanks to asshole epa he lost his job\\nAunt was a steno/typist. Lost her job to assholes at MSOffice\\nAnother uncle lost his job at Circuit City to assholes at Bestbuy and Amazon\\n\\nIt seems there were assholes every where',\n",
       "    'date': '2025-06-17',\n",
       "    'author': 'Myra222',\n",
       "    'upvoteCount': 93,\n",
       "    'commentCount': 10,\n",
       "    'replies': [{'text': 'The difference is that if humans replace humans, you can either be trained for that new role or your kids can have that new role. If AI replaces your job, it&apos;s gone forever.',\n",
       "      'date': '2025-06-17',\n",
       "      'author': 'FUIgotmine',\n",
       "      'upvoteCount': 89,\n",
       "      'commentCount': 0,\n",
       "      'replies': []},\n",
       "     {'text': 'Well maybe people could target those AI data center then. I ain&apos;t promoting anarchy tho',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'xSDB38',\n",
       "      'upvoteCount': 3,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'I wish AI was 10% as useful as people claimed it was.',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'Glunstan',\n",
       "    'upvoteCount': 51,\n",
       "    'commentCount': 3,\n",
       "    'replies': [{'text': '1000% accurate statement.\\n\\nI can talk to chatgpt all day long but noone lost their job for this to occur.\\n\\namazon tool that uses AI makes certain tasks take way longer.  I learned to shut up and just not use the tool and let manager get excited that others are &quot;using AI to do so and so task&quot;...',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'mambo_5',\n",
       "      'upvoteCount': 11,\n",
       "      'commentCount': 0,\n",
       "      'replies': []},\n",
       "     {'text': 'It depends on the industry you work on. Coding 100% makes you efficient. Wont replace coder but will make them efficient.',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'wPuk64',\n",
       "      'upvoteCount': 0,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'Just like the aashole coder who automated and removed manual labor',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'SubparPich',\n",
       "    'upvoteCount': 40,\n",
       "    'commentCount': 1,\n",
       "    'replies': [{'text': 'Haha',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'खुश मिज़ाज',\n",
       "      'upvoteCount': 1,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'PNLi05 in 1454: The printing press will result in net job loss\\n\\nPNLi05 in 1712: The steam engine will result in net job loss\\n\\nPNLi05 in 1785: The mechanical loom will result in net job loss\\n\\nPNLi05 in 1793: The cotton gin will result in net job loss\\n\\nPNLi05 in 1838: The hydraulic crane will result in net job loss\\n\\nPNLi05 in 1886: The automobile will result in net job loss\\n\\nPNLi05 in 1903: The airplane will result in net job loss\\n\\nPNLi05 in 1913: The moving assembly line will result in net job loss\\n\\nPNLi05 in 1967: The ATM will result in net job loss\\n\\nPNLi05 in 1983: The internet will result in net job loss',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'bipity',\n",
       "    'upvoteCount': 24,\n",
       "    'commentCount': 3,\n",
       "    'replies': [{'text': 'What an exhaustive list…. Amazing!',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'slow clap',\n",
       "      'upvoteCount': 4,\n",
       "      'commentCount': 0,\n",
       "      'replies': []},\n",
       "     {'text': 'Damn printing press stealing our jobs!',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'sqspify',\n",
       "      'upvoteCount': 0,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'Personally I am learning more electrical engineering/electronics through personal projects. If I won&apos;t spend as much time coding it&apos;s probably beneficial to learn more hardware based stuff.\\n\\nDigital isn&apos;t gonna replace physical space no matter how good AI gets. Buckle up dorks, time to get good at building stuff with your hands.',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'THT11',\n",
       "    'upvoteCount': 11,\n",
       "    'commentCount': 4,\n",
       "    'replies': [{'text': 'That’s only while the robotics progress lags behind the software, but you don’t think it’ll catch up soon? Keep in mind that AI will be humanity’s last invention. Once we reach AGI that is as good as PhD level workers across all domains, something that’s only 2-5yr away by most estimates, AI will then revolutionize every industry, robotics included, which means that human workers would simply be replaced by robots, across all job industries.\\n\\nNo point delaying the inevitable. Just save as much as you can now and start making plans to retire in a LCOL area where what you expect to have will be enough.',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'BlindBndit',\n",
       "      'upvoteCount': 3,\n",
       "      'commentCount': 0,\n",
       "      'replies': []},\n",
       "     {'text': 'Dude you watch too much I, Robot. AGI isn&apos;t coming. But those dipsh*t MBAs are trying their hardest to convince you it will.',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'THT11',\n",
       "      'upvoteCount': 8,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'You can blame the economic system we have where the goal is to make as much $ as cheaply as possible. \\nThe most expensive part of the whole system are the humans - if you can get the same results where humans/something can produce the same for less $ then you&apos;re incentivized to do so. \\nEvery tool that has helped labor/advance production has been a way of increasing profit without increasing laborers long term. \\nIt isn&apos;t AI - it&apos;s by design.',\n",
       "    'date': '2025-06-18',\n",
       "    'author': '1nuttyprof',\n",
       "    'upvoteCount': 17,\n",
       "    'commentCount': 3,\n",
       "    'replies': [{'text': 'Dumbest thing is shareholders are put first at the expense of employees',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'Wcaj46',\n",
       "      'upvoteCount': 4,\n",
       "      'commentCount': 0,\n",
       "      'replies': []},\n",
       "     {'text': 'Lol omg, companies operating more efficiently is good for everyone. How can you honestly be arguing for waste and inefficiency? We don&apos;t use humans for harvesting crops anymore and that&apos;s great cause machines do it way faster and we can get more food for a cheaper price. The same goes for AI.',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'vKfK17',\n",
       "      'upvoteCount': 1,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'What will AI write code for? Which customer? Which business if the consuming capabilities of current working force will become obselete as all of working force will be out of jobs..make it make sense the business runs because we go and spend that extra 2 grand every month...',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'lead_',\n",
       "    'upvoteCount': 11,\n",
       "    'commentCount': 1,\n",
       "    'replies': [{'text': 'Exactly op, for the economy to run you , money should exchange hands . If scores of people lose their jobs. Who is going to buy the product and services',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'NoLuck_09',\n",
       "      'upvoteCount': 8,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'I can understand this type of panic from non-tech workers, due to their lack of understanding of the tool. But if you work in tech and actually think this way, yeah, you&apos;re going to lose your job, but it won&apos;t have anything to do with AI. You&apos;re just a bad engineer.',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'jello7',\n",
       "    'upvoteCount': 15,\n",
       "    'commentCount': 4,\n",
       "    'replies': [{'text': 'Why do you say that? Don&apos;t u think what he&apos;s saying is partly true? Most entry level coding jobs will be lost!',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'eragon94',\n",
       "      'upvoteCount': 0,\n",
       "      'commentCount': 0,\n",
       "      'replies': []},\n",
       "     {'text': 'Na bro you don’t even know man. My startup is shipping so fast you big dogs are cooked',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'ffjj5',\n",
       "      'upvoteCount': 0,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'You want to know the hard truth?  \\nWe spent 2 decades telling people everyone  can code, and until now it was possible, you needed both really smart engineers but also code monkeys writing simple apis all day.  \\nTruth is a lot of developers are actually pretty bad, and write more by memory than logic.  \\nWell now the code monkeys are getting replaced.  \\nIf you are good and you adapt, you will stay.. simple as that',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'aerspyr',\n",
       "    'upvoteCount': 14,\n",
       "    'commentCount': 1,\n",
       "    'replies': [{'text': 'Yup',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'Zero 0',\n",
       "      'upvoteCount': 1,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]},\n",
       "   {'text': 'Hardware Engineer jobs will increase.\\nI am Wireless/HW engineer.\\nThanks AI.\\nMy job is safe',\n",
       "    'date': '2025-06-18',\n",
       "    'author': 'pro_trump',\n",
       "    'upvoteCount': 9,\n",
       "    'commentCount': 4,\n",
       "    'replies': [{'text': 'You aren&apos;t safe in the long term. But we are all safe in the sgort term as sw and hw engineers.',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'seniorsde',\n",
       "      'upvoteCount': 1,\n",
       "      'commentCount': 0,\n",
       "      'replies': []},\n",
       "     {'text': 'What makes you believe ai won’t eventually be able to engineer hardware?',\n",
       "      'date': '2025-06-18',\n",
       "      'author': 'JSjK23',\n",
       "      'upvoteCount': 1,\n",
       "      'commentCount': 0,\n",
       "      'replies': []}]}]},\n",
       " False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape_post(\"https://www.teamblind.com/post/any-layoffs-at-navan-in-dec-2024-46yqmj4g\")\n",
    "#scrape_post(\"https://www.teamblind.com/post/advice-needed-my-wife-just-returned-from-maternity-leavecompany-pushing-her-to-return-in-person-jvx4fqkh\")\n",
    "\n",
    "scrape_post(\"https://www.teamblind.com/post/ai-will-result-in-net-job-loss-l37jl741\")\n",
    "#scrape_post(\"https://www.teamblind.com/post/laid-off-right-before-parental-leave-starts-zvuwxlzt\")  # Test the function with the topic URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_new_posts(driver, processed_links, all_posts):\n",
    "    \"\"\"Process newly loaded posts and return stop reason (or None)\"\"\"\n",
    "    # Get all current post elements\n",
    "    current_elements = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]')\n",
    "    \n",
    "    new_posts = []\n",
    "    for element in current_elements:\n",
    "        try:\n",
    "            link = element.find_element(By.CSS_SELECTOR, 'a[data-testid=\"article-preview-click-box\"]')\n",
    "            href = link.get_attribute('href')\n",
    "            if href and href not in processed_links:\n",
    "                new_posts.append(href)\n",
    "                processed_links.add(href)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not new_posts:\n",
    "        return \"no_new_posts\"  # No new posts found\n",
    "    \n",
    "    print(f\"Processing {len(new_posts)} new posts...\")\n",
    "    \n",
    "    for i, post_url in enumerate(new_posts):\n",
    "        #print(f\"  Scraping post {i+1}/{len(new_posts)}\")\n",
    "        post_data, is_old = scrape_post(post_url)\n",
    "        \n",
    "        if is_old:\n",
    "            print(f\"Found old post ({post_url}), stopping processing\")\n",
    "            return \"old_post_found\"\n",
    "            \n",
    "        if post_data:\n",
    "            all_posts.append(post_data)\n",
    "            if len(all_posts) % 1000 == 0:\n",
    "                last_post_date = all_posts[-1]['date'] if all_posts else \"N/A\"\n",
    "                print(f\"Total posts scraped until now: {len(all_posts)}. Last post date: {last_post_date}\")\n",
    "            # Save results after each post\n",
    "            save_results(all_posts)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def save_results(posts):\n",
    "    \"\"\"Save results to JSON file incrementally\"\"\"\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_layoffs():\n",
    "    \"\"\"Main scraping function with incremental processing\"\"\"\n",
    "    all_posts = []\n",
    "    processed_links = set()\n",
    "    consecutive_no_new = 0\n",
    "    max_consecutive_no_new = 15  # Safety limit for no new posts\n",
    "    \n",
    "    # Set up Selenium driver\n",
    "    driver = setup_driver()\n",
    "    stop_reason = None\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading initial page...\")\n",
    "        driver.get(TOPIC_URL)\n",
    "        \n",
    "        # Wait for initial content\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "        )\n",
    "        \n",
    "        # Process initial batch of posts\n",
    "        reason = process_new_posts(driver, processed_links, all_posts)\n",
    "        if reason == \"old_post_found\":\n",
    "            stop_reason = \"Stopped due to old post in initial batch\"\n",
    "            return all_posts, stop_reason\n",
    "        \n",
    "        # Scroll and process incrementally\n",
    "        while True:\n",
    "            print(\"Scrolling to bottom...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2.5)  # Allow content to load\n",
    "            \n",
    "            # Process new posts\n",
    "            reason = process_new_posts(driver, processed_links, all_posts)\n",
    "            print(f\"Total posts scraped until now: {len(all_posts)}\")\n",
    "            if reason == \"old_post_found\":\n",
    "                stop_reason = \"Stopped due to old post found\"\n",
    "                print(stop_reason)\n",
    "                break\n",
    "            elif reason == \"no_new_posts\":\n",
    "                consecutive_no_new += 1\n",
    "                print(f\"No new posts detected ({consecutive_no_new}/{max_consecutive_no_new})\")\n",
    "                \n",
    "                # Break if we've had too many consecutive scrolls with no new posts\n",
    "                if consecutive_no_new >= max_consecutive_no_new:\n",
    "                    stop_reason = f\"Stopped after {max_consecutive_no_new} consecutive scrolls with no new posts\"\n",
    "                    print(stop_reason)\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_no_new = 0  # Reset counter if we found new posts\n",
    "        \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_posts, stop_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial page...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m posts, reason \u001b[38;5;241m=\u001b[39m scrape_layoffs()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#save_results(posts)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts to teamblind_layoffs_posts.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mscrape_layoffs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading initial page...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(TOPIC_URL)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Wait for initial content\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[1;32m     18\u001b[0m         EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle[data-testid=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle-preview-card\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:472\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    443\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 445\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m    145\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[1;32m    789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "posts, reason = scrape_layoffs()\n",
    "#save_results(posts)\n",
    "print(f\"Saved {len(posts)} posts to teamblind_layoffs_posts.json\")\n",
    "print(f\"Stopping reason: {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def process_comments(comments):\n",
    "    \"\"\"Recursively process comments and their replies\"\"\"\n",
    "    processed = []\n",
    "    for comment in comments:\n",
    "        try:\n",
    "            # Extract basic comment info\n",
    "            comment_data = {\n",
    "                \"text\": comment.get(\"text\", \"\"),\n",
    "                \"date\": comment.get(\"datePublished\", \"\")[:10],  # Only date part\n",
    "                \"author\": comment.get(\"author\", {}).get(\"name\", \"\") if \"author\" in comment else \"\",\n",
    "                \"upvoteCount\": comment.get(\"upvoteCount\", 0),\n",
    "                \"commentCount\": comment.get(\"commentCount\", 0),\n",
    "                \"replies\": []\n",
    "            }\n",
    "            \n",
    "            # Process nested replies if they exist\n",
    "            if \"comment\" in comment and comment[\"comment\"]:\n",
    "                comment_data[\"replies\"] = process_comments(comment[\"comment\"])\n",
    "            \n",
    "            processed.append(comment_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing comment: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return processed\n",
    "\n",
    "def scrape_single_post_2(url):\n",
    "    \"\"\"Scrape individual post page and return (data, is_old) tuple\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # if response.status_code == 429:\n",
    "        #     logger.warning(f\"Rate limited (429) on {url}\")\n",
    "\n",
    "\n",
    "        post_data = extract_json_ld(response.text)\n",
    "        if not post_data:\n",
    "            # SCRAPING_FAILED += 1\n",
    "            # append_failed_url(url)  \n",
    "            print(f\"No JSON-LD data found for {url}\")\n",
    "            return None, False\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        if post_date < MIN_DATE: return None, True  # Post is too old\n",
    "\n",
    "        # Extract post details\n",
    "        post_details = extract_post_details(response.text)\n",
    "\n",
    "        # Extract and format comments\n",
    "        comments = []\n",
    "        if \"comment\" in post_data:\n",
    "            try:\n",
    "                comments = process_comments(post_data[\"comment\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing comments: {str(e)}\")\n",
    "        else:\n",
    "            print(\"No comments element found in post data\")\n",
    "\n",
    "        result = {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_date_str,  # Store only the date part\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"author\": post_details[\"author_id\"], \n",
    "            \"authorCompany\": post_details[\"author_company\"],\n",
    "            \"likeCount\": post_details[\"like_count\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"],\n",
    "            \"viewCount\": post_details[\"view_count\"],\n",
    "            \"comments\": comments\n",
    "        }\n",
    "\n",
    "        # # Append to output immediately\n",
    "        # append_to_output(result)\n",
    "        # SCRAPING_SUCCESSFULL += 1\n",
    "\n",
    "        # if SCRAPING_SUCCESSFULL % 200 == 0:\n",
    "        #     logger.info(f\"Successfully scraped {SCRAPING_SUCCESSFULL} posts so far\")\n",
    "        #     logger.info(f\"last scraped post date: {result['post_date_str']}\")\n",
    "        return result, False  # Post is not old\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping https://www.teamblind.com/post/laid-off-right-before-parental-leave-starts-zvuwxlzt: name 'HEADERS' is not defined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_single_post_2(\"https://www.teamblind.com/post/laid-off-right-before-parental-leave-starts-zvuwxlzt\")  # Test the function with the topic URL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
