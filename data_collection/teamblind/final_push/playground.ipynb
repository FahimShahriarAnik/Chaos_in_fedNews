{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "MIN_DATE = date(2025, 6, 10)  # Use simple date object (no time)\n",
    "BASE_URL = \"https://www.teamblind.com\"\n",
    "TOPIC_URL = f\"{BASE_URL}/topics/General-Topics/Layoffs\"\n",
    "OUTPUT_FILE = 'posttesting.json'  # JSON Lines format\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure Selenium\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,800\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Network.setUserAgentOverride\",\n",
    "        {\"userAgent\": HEADERS[\"User-Agent\"]}\n",
    "    )\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_ld(html):\n",
    "    \"\"\"Extract JSON-LD data from post page\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    script = soup.find('script', {'id': 'article-discussion-forum-posting-schema', 'type': 'application/ld+json'})\n",
    "    \n",
    "    if not script:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        return json.loads(script.string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_post_details(html_text):\n",
    "    \"\"\"\n",
    "    Extract post details from the HTML response text.\n",
    "    \n",
    "    Args:\n",
    "        html_text (str): The HTML content of the page as a string.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted details:\n",
    "              - like_count (int)\n",
    "              - view_count (int)\n",
    "              - author_company (str)\n",
    "              - author_id (str)\n",
    "              - button_container (bool) (True if found, False otherwise)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    \n",
    "    # Find the button container\n",
    "    button_container = soup.find('div', class_='flex gap-2 md:gap-4')\n",
    "    \n",
    "    # Initialize counts\n",
    "    like_count = 0\n",
    "    view_count = 0\n",
    "    if button_container:\n",
    "        # Find like button by aria-label\n",
    "        like_button = button_container.find('button', {'aria-label': 'Like this post'})\n",
    "        if like_button and 'data-count' in like_button.attrs:\n",
    "            try:\n",
    "                like_count = int(like_button['data-count'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Find view button by aria-label\n",
    "        view_button = button_container.find('button', {'aria-label': 'Views'})\n",
    "        if view_button and 'data-count' in view_button.attrs:\n",
    "            try:\n",
    "                view_count = int(view_button['data-count'])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    # Extract author details\n",
    "    author_company = \"\"\n",
    "    author_id = \"\"\n",
    "    author_div = soup.find('div', class_='flex h-full items-center text-xs text-gray-800')\n",
    "    if author_div:\n",
    "        # Extract company name\n",
    "        company_link = author_div.find('a')\n",
    "        if company_link:\n",
    "            author_company = company_link.get_text(strip=True)\n",
    "        \n",
    "        # Extract author ID - find the last text node in the div\n",
    "        text_nodes = [text for text in author_div.stripped_strings]\n",
    "        if text_nodes:\n",
    "            # Author ID is the last text node after the SVG\n",
    "            author_id = text_nodes[-1]\n",
    "    \n",
    "    return {\n",
    "        \"like_count\": like_count,\n",
    "        \"view_count\": view_count,\n",
    "        \"author_company\": author_company,\n",
    "        \"author_id\": author_id,\n",
    "        \"button_container\": bool(button_container)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_post(url):\n",
    "    \"\"\"Scrape individual post page and return (data, is_old) tuple\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        post_data = extract_json_ld(response.text)\n",
    "        if not post_data:\n",
    "            print(f\"No JSON-LD data found for {url}\")\n",
    "            return None, False\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        if post_date < MIN_DATE: return None, True  # Post is too old\n",
    "\n",
    "        # Extract post details\n",
    "        post_details = extract_post_details(response.text)\n",
    "\n",
    "        return {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_date_str,  # Store only the date part\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"author\": post_details[\"author_id\"], \n",
    "            \"authorCompany\": post_details[\"author_company\"],\n",
    "            \"likeCount\": post_details[\"like_count\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"],\n",
    "            \"viewCount\": post_details[\"view_count\"],\n",
    "        }, False\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'headline': 'Laid off right before parental leave starts',\n",
       "  'text': 'My firm offers 10 weeks of parental (paternal) leave. I always had a good relationship with my manager (or at least I thought so), excellent review ratings, good standing among teammates. But the caveat is Iâ€™m the only IC whoâ€™s remote with one more manager but he frequently travels onsite (like once every two months).\\n\\nI initially took 3 weeks off earlier this year and assured my manager I wouldnâ€™t leave longer stretches and overburden the team. Now I submitted another 2 weeks which was approved but was blindsided last week when he moved our 1-1 from Wed to Fri (he happens to do that at times) and I saw the HR on the call which is when he broke the news knowing Monday is when my parental leave starts. They did offer me 2 months severance though\\n\\nFeeling betrayed ðŸ˜’',\n",
       "  'date': '2025-06-22',\n",
       "  'url': 'https://www.teamblind.com/post/laid-off-right-before-parental-leave-starts-zvuwxlzt',\n",
       "  'author': 'ohdbdj9',\n",
       "  'authorCompany': 'ex-Amazon',\n",
       "  'likeCount': 8,\n",
       "  'commentCount': 29,\n",
       "  'viewCount': 942},\n",
       " False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape_post(\"https://www.teamblind.com/post/any-layoffs-at-navan-in-dec-2024-46yqmj4g\")\n",
    "#scrape_post(\"https://www.teamblind.com/post/advice-needed-my-wife-just-returned-from-maternity-leavecompany-pushing-her-to-return-in-person-jvx4fqkh\")\n",
    "\n",
    "\n",
    "scrape_post(\"https://www.teamblind.com/post/laid-off-right-before-parental-leave-starts-zvuwxlzt\")  # Test the function with the topic URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_new_posts(driver, processed_links, all_posts):\n",
    "    \"\"\"Process newly loaded posts and return stop reason (or None)\"\"\"\n",
    "    # Get all current post elements\n",
    "    current_elements = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]')\n",
    "    \n",
    "    new_posts = []\n",
    "    for element in current_elements:\n",
    "        try:\n",
    "            link = element.find_element(By.CSS_SELECTOR, 'a[data-testid=\"article-preview-click-box\"]')\n",
    "            href = link.get_attribute('href')\n",
    "            if href and href not in processed_links:\n",
    "                new_posts.append(href)\n",
    "                processed_links.add(href)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not new_posts:\n",
    "        return \"no_new_posts\"  # No new posts found\n",
    "    \n",
    "    print(f\"Processing {len(new_posts)} new posts...\")\n",
    "    \n",
    "    for i, post_url in enumerate(new_posts):\n",
    "        #print(f\"  Scraping post {i+1}/{len(new_posts)}\")\n",
    "        post_data, is_old = scrape_post(post_url)\n",
    "        \n",
    "        if is_old:\n",
    "            print(f\"Found old post ({post_url}), stopping processing\")\n",
    "            return \"old_post_found\"\n",
    "            \n",
    "        if post_data:\n",
    "            all_posts.append(post_data)\n",
    "            if len(all_posts) % 1000 == 0:\n",
    "                last_post_date = all_posts[-1]['date'] if all_posts else \"N/A\"\n",
    "                print(f\"Total posts scraped until now: {len(all_posts)}. Last post date: {last_post_date}\")\n",
    "            # Save results after each post\n",
    "            save_results(all_posts)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def save_results(posts):\n",
    "    \"\"\"Save results to JSON file incrementally\"\"\"\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_layoffs():\n",
    "    \"\"\"Main scraping function with incremental processing\"\"\"\n",
    "    all_posts = []\n",
    "    processed_links = set()\n",
    "    consecutive_no_new = 0\n",
    "    max_consecutive_no_new = 15  # Safety limit for no new posts\n",
    "    \n",
    "    # Set up Selenium driver\n",
    "    driver = setup_driver()\n",
    "    stop_reason = None\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading initial page...\")\n",
    "        driver.get(TOPIC_URL)\n",
    "        \n",
    "        # Wait for initial content\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "        )\n",
    "        \n",
    "        # Process initial batch of posts\n",
    "        reason = process_new_posts(driver, processed_links, all_posts)\n",
    "        if reason == \"old_post_found\":\n",
    "            stop_reason = \"Stopped due to old post in initial batch\"\n",
    "            return all_posts, stop_reason\n",
    "        \n",
    "        # Scroll and process incrementally\n",
    "        while True:\n",
    "            print(\"Scrolling to bottom...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2.5)  # Allow content to load\n",
    "            \n",
    "            # Process new posts\n",
    "            reason = process_new_posts(driver, processed_links, all_posts)\n",
    "            print(f\"Total posts scraped until now: {len(all_posts)}\")\n",
    "            if reason == \"old_post_found\":\n",
    "                stop_reason = \"Stopped due to old post found\"\n",
    "                print(stop_reason)\n",
    "                break\n",
    "            elif reason == \"no_new_posts\":\n",
    "                consecutive_no_new += 1\n",
    "                print(f\"No new posts detected ({consecutive_no_new}/{max_consecutive_no_new})\")\n",
    "                \n",
    "                # Break if we've had too many consecutive scrolls with no new posts\n",
    "                if consecutive_no_new >= max_consecutive_no_new:\n",
    "                    stop_reason = f\"Stopped after {max_consecutive_no_new} consecutive scrolls with no new posts\"\n",
    "                    print(stop_reason)\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_no_new = 0  # Reset counter if we found new posts\n",
    "        \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_posts, stop_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial page...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m posts, reason \u001b[38;5;241m=\u001b[39m scrape_layoffs()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#save_results(posts)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts to teamblind_layoffs_posts.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mscrape_layoffs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading initial page...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(TOPIC_URL)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Wait for initial content\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[1;32m     18\u001b[0m         EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle[data-testid=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle-preview-card\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     19\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:472\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    443\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 445\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m    145\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[1;32m    789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "posts, reason = scrape_layoffs()\n",
    "#save_results(posts)\n",
    "print(f\"Saved {len(posts)} posts to teamblind_layoffs_posts.json\")\n",
    "print(f\"Stopping reason: {reason}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
