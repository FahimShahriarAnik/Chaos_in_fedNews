{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (2.11.1)\n",
      "Requirement already satisfied: scrapy-playwright in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (0.0.43)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (43.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (24.2.1)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (1.21.0)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: setuptools in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (75.1.0)\n",
      "Requirement already satisfied: packaging in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (24.1)\n",
      "Requirement already satisfied: tldextract in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (5.1.2)\n",
      "Requirement already satisfied: lxml>=4.4.1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (5.2.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: playwright>=1.15 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from scrapy-playwright) (1.52.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: pyee<14,>=13 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from playwright>=1.15->scrapy-playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from playwright>=1.15->scrapy-playwright) (3.2.3)\n",
      "Requirement already satisfied: six in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Requirement already satisfied: pyasn1-modules in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (4.11.0)\n",
      "Requirement already satisfied: idna in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: pycparser in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.8.30)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement blindcrawler (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for blindcrawler\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scrapy scrapy-playwright\n",
    "!playwright install chromium\n",
    "!pip install blindcrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: blindcrawler\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show blindcrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'blindcrawler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerRunner\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproject\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_project_settings\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mblindcrawler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspiders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayoffs_spider\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayoffsSpider\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_spider\u001b[39m():\n\u001b[1;32m     79\u001b[0m     runner \u001b[38;5;241m=\u001b[39m CrawlerRunner(get_project_settings())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'blindcrawler'"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "from scrapy_playwright.page import PageMethod\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "class LayoffsSpider(scrapy.Spider):\n",
    "    name = \"layoffs\"\n",
    "    allowed_domains = [\"teamblind.com\"]\n",
    "    start_urls = [\"https://www.teamblind.com/topics/General-Topics/Layoffs\"]\n",
    "\n",
    "    custom_settings = {\n",
    "        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n",
    "        \"DOWNLOAD_HANDLERS\": {\n",
    "            \"http\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "            \"https\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "        },\n",
    "        \"PLAYWRIGHT_LAUNCH_OPTIONS\": {\"headless\": False},\n",
    "    }\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(\n",
    "            url=self.start_urls[0],\n",
    "            meta={\n",
    "                \"playwright\": True,\n",
    "                \"playwright_include_page\": True,\n",
    "                \"playwright_page_methods\": [\n",
    "                    PageMethod(\"wait_for_selector\", \".topic-list\"),  ############### adjust selector as needed\n",
    "                ],\n",
    "            },\n",
    "            callback=self.parse,\n",
    "        )\n",
    "\n",
    "    async def parse(self, response):\n",
    "        page = response.meta[\"playwright_page\"]\n",
    "\n",
    "        # infinite scroll logic\n",
    "        prev_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(1500)\n",
    "            new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if new_height == prev_height:\n",
    "                break\n",
    "            prev_height = new_height\n",
    "\n",
    "        # extract each post (DOM selectors need adjusting)\n",
    "        sel = Selector(text=await page.content())\n",
    "        posts = sel.css(\".topic-item\")  ############### update selector\n",
    "        cutoff = datetime.now() - timedelta(days=150)\n",
    "        for post in posts:\n",
    "            date_str = post.css(\".timestamp::text\").get()\n",
    "            ################ you may need to parse different formats\n",
    "            try:\n",
    "                post_date = datetime.fromisoformat(date_str)\n",
    "            except:\n",
    "                continue\n",
    "            if post_date < cutoff:\n",
    "                continue\n",
    "            title = post.css(\".topic-title::text\").get()\n",
    "            link = post.css(\"a::attr(href)\").get()\n",
    "            self.log(f\"{post_date.date()}  {title}  {link}\")\n",
    "\n",
    "        await page.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess(get_project_settings())\n",
    "    process.crawl(LayoffsSpider)  # or process.crawl('layoffs')\n",
    "    process.start()  # blocks until spider finishes\n",
    "\n",
    "# import asyncio\n",
    "# from scrapy.crawler import CrawlerRunner\n",
    "# from scrapy.utils.project import get_project_settings\n",
    "# from blindcrawler.spiders.layoffs_spider import LayoffsSpider\n",
    "\n",
    "# async def run_spider():\n",
    "#     runner = CrawlerRunner(get_project_settings())\n",
    "#     await runner.crawl(LayoffsSpider)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(run_spider())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Length: 1555\n",
      "CF-Ray Header: None\n",
      "Set-Cookie Header: None\n",
      "\n",
      "robots.txt Restrictions:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta data-hid=\"viewport\" name=\"viewport\" content=\"user-scalable=no, initial-scale=1, width=device-width, minimal-ui, maximum-scale=1\" />\n",
      "\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n",
      "\n",
      "    <title>Your Anonymous Workplace Community - Blind</title>\n",
      "    <link data-n-head=\"true\" rel=\"icon\" type=\"image/x-icon\" href=\"/favicon.ico\" />\n",
      "\n",
      "    <link rel=\"stylesheet\" type=\"text/css\" href=\"https://s3-us-west-2.amazonaws.com/www.teamblind.com/error-pages/static-blockpage-last/css/etc.min.css\">\n",
      "</head>\n",
      "<body>\n",
      "\n",
      "<div class=\"flex_layout\">\n",
      "    <!-- fnc_ux -->\n",
      "    <header>\n",
      "        <div class=\"wrap\">\n",
      "            <h2 class=\"h_blind\"><!--add_topic -->\n",
      "                <a href=\"#\"><i class=\"blind\">blind</i></a>\n",
      "            </h2>\n",
      "        </div>\n",
      "    </header>\n",
      "    <div id=\"container\" role=\"main\">\n",
      "        <div class=\"max_wrap\">\n",
      "\n",
      "            <div class=\"error\">\n",
      "                <div class=\"msg_area block\">\n",
      "                    <span class=\"ico ico_notice_cicle\"><em class=\"blind\">notification</em></span>\n",
      "                    Oops! Something went wrong. <br class=\"is_mobile\"/>Please try again later.<br/>\n",
      "                    If the problem continues, please contact our team.<br />\n",
      "                    <a href=\"mailto:blindapp@teamblind.com\">US : blindapp@teamblind.com</a><br />\n",
      "                    <a href=\"mailto:support@teamblind.com\">KR : support@teamblind.com</a>\n",
      "                </div>\n",
      "            </div>\n",
      "\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <!-- footer_dummy -->\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Check Cloudflare protection\n",
    "response = requests.get('https://www.teamblind.com/topics/General-Topics/Layoffs', timeout=10)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Response Length: {len(response.text)}\")\n",
    "print(f\"CF-Ray Header: {response.headers.get('CF-Ray')}\")  # Indicates Cloudflare\n",
    "print(f\"Set-Cookie Header: {response.headers.get('Set-Cookie')}\")  # Look for __cf_bm\n",
    "\n",
    "# Check robots.txt\n",
    "robots = requests.get('https://www.teamblind.com/robots.txt').text\n",
    "print(\"\\nrobots.txt Restrictions:\")\n",
    "print(robots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Length: 780836\n",
      "CF-Ray Header: None\n",
      "Set-Cookie: None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.teamblind.com/topics/General-Topics/Layoffs\"\n",
    "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Response Length: {len(response.text)}\")\n",
    "print(f\"CF-Ray Header: {response.headers.get('CF-Ray')}\")\n",
    "print(f\"Set-Cookie: {response.headers.get('Set-Cookie')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "  Scraping post 1/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Joining Google now', 'text': 'Is it risky to join right now? I assume it&apos;s possible to get layoff a few months after joining... assuming i have other offers and existing job.', 'datePublished': '2025-06-11T04:00:00.000Z', 'url': 'https://www.teamblind.com/post/joining-google-now-2zzbgmne', 'author': {'@type': 'Person', 'identifier': 'WiBg03', 'name': 'WiBg03'}, 'commentCount': 135, 'comment': [{'@type': 'Comment', 'text': '1. G does not have that kind of safety net anymore\\n2. Culture has gone down to shit with Amazon like PiP systems\\n3. It&apos;s more or less a sweatshop with focus on getting done more with less\\n\\nYou make the call', 'datePublished': '2025-06-11T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Do you work in GCP? I thought anything non-cloud related is fine', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Gigajesh', 'name': 'Gigajesh'}, 'upvoteCount': 4, 'commentCount': 0}, {'@type': 'Comment', 'text': 'This!', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Joqf43', 'name': 'Joqf43'}, 'upvoteCount': 2, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'SubparPich', 'name': 'SubparPich'}, 'upvoteCount': 101, 'commentCount': 15}, {'@type': 'Comment', 'text': 'big tech is infested with layoff disease\\n\\nif you prioritize stability and have something more stable...', 'datePublished': '2025-06-11T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'nice and stable minimum wage Walmart greeter', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'OSVu55', 'name': 'OSVu55'}, 'upvoteCount': 9, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Help define stability? Like what kind of company , anything specific we should be looking at ?', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'apisucker ', 'name': 'apisucker '}, 'upvoteCount': 1, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'GOLDB@N@N@', 'name': 'GOLDB@N@N@'}, 'upvoteCount': 39, 'commentCount': 6}, {'@type': 'Comment', 'text': 'Most of those voting not to join are just jealous they don&apos;t work here 😏', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Nope. It really sucks working at G. The good one leaves for better companies and the coaster/mediocre stay because they can’t leave.', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': '- -.-.', 'name': '- -.-.'}, 'upvoteCount': 9, 'commentCount': 0}, {'@type': 'Comment', 'text': 'No, those are employees who know what&apos;s cooking inside and how bad it is here', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'O(log²n)', 'name': 'O(log²n)'}, 'upvoteCount': 4, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'pluralityi', 'name': 'pluralityi'}, 'upvoteCount': 28, 'commentCount': 9}, {'@type': 'Comment', 'text': 'No company is safe. Layoffs are part of life now. Just maintain some emergency fund and as long as they lay you off with a severance, you are good. Just take the offer and just concentrate on good work and the best working relationship with your manager.', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': '^^ this', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'hurhel', 'name': 'hurhel'}, 'upvoteCount': 4, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Well said', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Lettuce169', 'name': 'Lettuce169'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'BL(IN)DGuy', 'name': 'BL(IN)DGuy'}, 'upvoteCount': 49, 'commentCount': 2}, {'@type': 'Comment', 'text': 'WTF is this? You don’t have an offer, you don’t have nothing right now, not even a TC, and asking to join or not? At least provide your TC or GTFO', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Calm down', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'olaju2', 'name': 'olaju2'}, 'upvoteCount': 15, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Yes, please take your meds', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': '(eqhk9&amp;&amp;', 'name': '(eqhk9&amp;&amp;'}, 'upvoteCount': 5, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'libertyAll', 'name': 'libertyAll'}, 'upvoteCount': 21, 'commentCount': 2}, {'@type': 'Comment', 'text': 'No no no. This is not the google we knew any more. Last in first out. Bad time.', 'datePublished': '2025-06-11T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'What about Microsoft? 🤣🤣🤣', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'teamninja', 'name': 'teamninja'}, 'upvoteCount': 5, 'commentCount': 0}, {'@type': 'Comment', 'text': 'They operate by FIFO. 30 year veterans are getting shit canned', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Gigajesh', 'name': 'Gigajesh'}, 'upvoteCount': 8, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'nq77jwp', 'name': 'nq77jwp'}, 'upvoteCount': 25, 'commentCount': 2}, {'@type': 'Comment', 'text': 'It became much shittier, but is likely still less shitty than most other companies (that became shittier too).', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'This ^', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'musk elon', 'name': 'musk elon'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'WlJW12', 'name': 'WlJW12'}, 'upvoteCount': 28, 'commentCount': 1}, {'@type': 'Comment', 'text': 'Google company will collapse. They are losing search and ad revenue. Wouldnt join if they offered me 10x my salary.', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Lol seriously? If you lasted just 1 year it would be worth 10 years at your current job. I would take that offer any day, at any company', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'vYaS21', 'name': 'vYaS21'}, 'upvoteCount': 20, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Even Amazon?', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'L6SDE450K', 'name': 'L6SDE450K'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'aigr8gglsx', 'name': 'aigr8gglsx'}, 'upvoteCount': 2, 'commentCount': 15}, {'@type': 'Comment', 'text': 'Why do yall keep targeting tech? Do folks not realize other industries pay pretty damn well and aren&apos;t laying off at the same rate?', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Enlighten me, which industries?', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'NPC27399', 'name': 'NPC27399'}, 'upvoteCount': 7, 'commentCount': 0}, {'@type': 'Comment', 'text': 'I wanna know, too! I spent 9 months looking and nothing I found paid as well as tech.', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'BrdU75', 'name': 'BrdU75'}, 'upvoteCount': 1, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'qbnx00', 'name': 'qbnx00'}, 'upvoteCount': 3, 'commentCount': 8}, {'@type': 'Comment', 'text': 'I had heard so much about it, culture and work life and what not. All bs. Go for something that is giving you decent money, a little peace of mind with autonomy and a good bunch of people.', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Howz your experience working at uber? Howz the culture', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Marcos9', 'name': 'Marcos9'}, 'upvoteCount': 1, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'randomese', 'name': 'randomese'}, 'upvoteCount': 19, 'commentCount': 1}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 10}}\n",
      "  Scraping post 2/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Should I mention mass layoff on resume? ', 'text': 'So I’m about 5 years into my career now and have worked at 4 great big tech companies. However, I’ve also been unlucky and have been part of 2 mass layoffs due to downsizing.\\n\\nNow I’m wondering how this make my resume look from a recruiters perspective when they don’t have all the context - is this “normal” given the volatility in the tech market since Covid? Does this reduce my chances of getting an interview?\\n\\nAlso, should I maybe include somewhere in my resume that it was a mass layoff?\\n\\nAny feedback would be much appreciated!', 'datePublished': '2025-06-16T21:00:00.000Z', 'url': 'https://www.teamblind.com/post/should-i-mention-mass-layoff-on-resume-dwxwiznw', 'author': {'@type': 'Person', 'identifier': 'Hi Tech', 'name': 'Hi Tech'}, 'commentCount': 10, 'comment': [{'@type': 'Comment', 'text': 'Yes, just put layoff next to your tenure I.e\\n\\nAmerican Express Jan 25 - July 25 (layoff)', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Does that help tho? It’s either saying laid off or they think I’m a job hopper… idk what’s best', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Hi Tech', 'name': 'Hi Tech'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'I don’t think I would put layoff on a resume… Leave that for the 1:1 conversations.', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'EIhk74', 'name': 'EIhk74'}, 'upvoteCount': 4, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'nsLF80', 'name': 'nsLF80'}, 'upvoteCount': 1, 'commentCount': 2}, {'@type': 'Comment', 'text': 'CA employment lawyer here. If you do, I would refer to it as a Reduction in Force (RIF).', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'SFWorkLaw', 'name': 'SFWorkLaw'}, 'upvoteCount': 2, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Never. For what/whose benefit?\\nDont.', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Well what’s worst? Saying you were part of a mass layoff or them thinking you’re a job hopper?', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Hi Tech', 'name': 'Hi Tech'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Either, doesnt help - your resume goes straight to trash, &amp; especially once in market &amp; circulated, you cant take it back. Dont.\\nYou didnt answer-why?', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': '-life-', 'name': '-life-'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': '-life-', 'name': '-life-'}, 'upvoteCount': 0, 'commentCount': 2}, {'@type': 'Comment', 'text': 'Does that help tho? It’s either saying laid off or they think I’m a job hopper… idk what’s best', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'What&apos;s your technology? Just curious why in 2 layoffs!', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'GenZzzzz', 'name': 'GenZzzzz'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'Hi Tech', 'name': 'Hi Tech'}, 'upvoteCount': 0, 'commentCount': 1}, {'@type': 'Comment', 'text': 'Newer ever openly show to hr any real &quot;negative&quot; about your career.\\nHR is not your friend and doesn&apos;t deserve even a bit of honesty.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'OiUD80', 'name': 'OiUD80'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 1}}\n",
      "  Scraping post 3/22\n",
      "post_data: None\n",
      "  Scraping post 4/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'AI will result in net job loss', 'text': 'Thanks to asshole AI coders eventually humans will lose jobs to fking AI', 'datePublished': '2025-06-17T18:00:00.000Z', 'url': 'https://www.teamblind.com/post/ai-will-result-in-net-job-loss-l37jl741', 'author': {'@type': 'Person', 'identifier': 'PNLi05', 'name': 'PNLi05'}, 'commentCount': 3, 'comment': [{'@type': 'Comment', 'text': 'Grandpa was a whaler and produced whale oil for lamps. Thanks to asshole oil companies he lost his job\\nUncle was a coal miner. Thanks to asshole epa he lost his job\\nAunt was a steno/typist. Lost her job to assholes at MSOffice\\nAnother uncle lost his job at Circuit City to assholes at Bestbuy and Amazon\\n\\nIt seems there were assholes every where', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Myra222', 'name': 'Myra222'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Come AI coder then?', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'xgJd83', 'name': 'xgJd83'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Leaner teams', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'TinTin2', 'name': 'TinTin2'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 1}}\n",
      "  Scraping post 5/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Any layoffs at Navan in Dec 2024? ', 'text': 'Trying to verify if any teams were affected late last year (Dec 2024). Was this a quiet lay offs or only PiPs?', 'datePublished': '2025-06-17T16:00:00.000Z', 'url': 'https://www.teamblind.com/post/any-layoffs-at-navan-in-dec-2024-46yqmj4g', 'author': {'@type': 'Person', 'identifier': 's8101 ', 'name': 's8101 '}, 'commentCount': 2, 'comment': [{'@type': 'Comment', 'text': 'Yes! And filed for unemployment benefits.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 's8101 ', 'name': 's8101 '}, 'upvoteCount': 2, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Have a candidate that told you they were laid off? lol', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'rivjehdw15', 'name': 'rivjehdw15'}, 'upvoteCount': 1, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 0}}\n",
      "  Scraping post 6/22\n",
      "post_data: None\n",
      "  Scraping post 7/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Advise for DevOps Career', 'text': 'Hi Blind,\\n\\nIm currently employed with a company that is announcing layoffs soon. I was informed that I would be retained with no change to salary or benefits. My counterpart, who has been there longer, is being let go.I believe it&apos;s inevitable that I&apos;ll be let go and want to be prepared for the market, no matter how bad it is.\\n\\nI&apos;m a senior level DevOps Engineer which could mean anything, but I primarily work in Azure, manging the entire stack from Infrastructure to SDLC. I have 6 YOE in DevOps with a Systems Engineering background (2 YOE). I have a business and CIS degree, currently only one low level Security cert as I&apos;ve let all the others expire.\\n\\nCurrently pursuing projects with AI.\\n\\nI will fill in the gaps at my current role, but where should I be skilling up to ensure a steady income in the future?\\n\\nYOE: 8 COMP: 170k', 'datePublished': '2025-06-17T14:00:00.000Z', 'url': 'https://www.teamblind.com/post/advise-for-devops-career-xcp2ffle', 'author': {'@type': 'Person', 'identifier': 'xanderous', 'name': 'xanderous'}, 'commentCount': 0, 'comment': [], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 0}}\n",
      "  Scraping post 8/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'HR/Recruiting Jobs', 'text': 'The market is terrible for HR and recruiting. Still looking for a job. What are folks in my industry doing? Any pointers? #severance #layoff #HR #jobseeker #HRjobs #humanresources', 'datePublished': '2025-06-16T22:00:00.000Z', 'url': 'https://www.teamblind.com/post/hrrecruiting-jobs-05kilw78', 'author': {'@type': 'Person', 'identifier': 'hust13r', 'name': 'hust13r'}, 'commentCount': 2, 'comment': [{'@type': 'Comment', 'text': 'DM for Zillow if you find something you’d want a referral for.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'nepobby ', 'name': 'nepobby '}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Many people changed career, doing something different like Sales or take Masters degree in some other fields, some even become nurse/pt/ot etc.. Some people moved to non Tech HR / Recruiting like banks, retails, real estate etc', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'gydgcshnk', 'name': 'gydgcshnk'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 0}}\n",
      "  Scraping post 9/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Microsoft / Motive / eBay / Dropbox Referral Request ', 'text': 'Hey everyone,\\n\\nHope you all are good. Was wondering if anyone who works at these companies could help  me with employee referral? I am seasoned finance professional who is interested in the job openings posted at these companies. I would like to request for referrals and can DM all the details.\\n\\neBay\\nMicrosoft \\nMotive\\nDropbox\\n\\nWould highly appreciate the help. Thanks so much in advance!\\n\\n#eBay #referral  #finance #tech #layoff #severance #fintros #jobhunt #career #financial #help #motive #microsoft #dropbox', 'datePublished': '2025-06-16T21:00:00.000Z', 'url': 'https://www.teamblind.com/post/microsoft-motive-ebay-dropbox-referral-request-lwc6rql6', 'author': {'@type': 'Person', 'identifier': 'Fintechbay', 'name': 'Fintechbay'}, 'commentCount': 2, 'comment': [{'@type': 'Comment', 'text': 'If you’re interested in Zillow or find a role you’d want to apply for, feel free to DM.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'nepobby ', 'name': 'nepobby '}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'DM if you see a fit at TikTok / Bytedance', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'fazc04', 'name': 'fazc04'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 0}}\n",
      "  Scraping post 10/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Cut from MSFT at 1 YOE, want NYC for next job.', 'text': 'How fucked am I?\\n\\nHow precarious is the situation for me to be forced back into the market? I’m looking to recover similar TC while in NYC ideally.\\n\\nI’m in Boston purely for Microsoft and I’m sick of not having my SO, core friends and family during this time.\\n\\nWould I be able to recover a similar TC with this YOE?\\n\\nTC: $180K', 'datePublished': '2025-06-16T18:00:00.000Z', 'url': 'https://www.teamblind.com/post/cut-from-msft-at-1-yoe-want-nyc-for-next-job-czaa0eee', 'author': {'@type': 'Person', 'identifier': 'dudebro198', 'name': 'dudebro198'}, 'commentCount': 3, 'comment': [{'@type': 'Comment', 'text': 'Only option is to try.  Nobody here knows for sure, we&apos;re all making the same educated guess you are,', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'LeifOTWind', 'name': 'LeifOTWind'}, 'upvoteCount': 3, 'commentCount': 0}, {'@type': 'Comment', 'text': 'No big deal. Focus on what you have control on.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'c++fe', 'name': 'c++fe'}, 'upvoteCount': 2, 'commentCount': 0}, {'@type': 'Comment', 'text': 'One way to find out…..', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'PmqI06', 'name': 'PmqI06'}, 'upvoteCount': 1, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 1}}\n",
      "  Scraping post 11/22\n",
      "post_data: None\n",
      "  Scraping post 12/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Is layoff finally over? Fed signaling rate cut coming soon', 'text': 'Not seeing much layoff news recently', 'datePublished': '2025-06-16T16:00:00.000Z', 'url': 'https://www.teamblind.com/post/is-layoff-finally-over-fed-signaling-rate-cut-coming-soon-g124y0e3', 'author': {'@type': 'Person', 'identifier': 'rokuukor', 'name': 'rokuukor'}, 'commentCount': 12, 'comment': [{'@type': 'Comment', 'text': 'I think all the jobs that have gone to India ain&apos;t coming back unfortunately.', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'PocariusVi', 'name': 'PocariusVi'}, 'upvoteCount': 11, 'commentCount': 0}, {'@type': 'Comment', 'text': 'We just had 5% layoff at Zscaler', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'noscaler', 'name': 'noscaler'}, 'upvoteCount': 7, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Just had 3% layoffs in msft few weeks back and new rumor of bigger layoffs in coming weeks', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'bubblint', 'name': 'bubblint'}, 'upvoteCount': 7, 'commentCount': 0}, {'@type': 'Comment', 'text': 'link where fed signaling rate cut?', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'CrazyTc', 'name': 'CrazyTc'}, 'upvoteCount': 5, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Elections have consequences. I hope you&apos;re not tired of winning already, because we&apos;re just starting.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'The layoff waves started in 2022, 2023 when Biden was in office', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'rokuukor', 'name': 'rokuukor'}, 'upvoteCount': 4, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'MHxz10', 'name': 'MHxz10'}, 'upvoteCount': 0, 'commentCount': 1}, {'@type': 'Comment', 'text': 'The ongoing tech layoffs are very weakly related to interest rates. Per J. Powell ~88% of all outstanding loans, etc, are at pre covid  rates (that&apos;s why hiking rates from 0 -&amp;gt 5.5% didn&apos;t impact the economy adversely, aka recession)\\n\\nDon&apos;t live under any illusion that rate cuts will somehow stop layoffs or spur  &quot;tech&quot; jobs. The job market is saturated, and AI investment needs to start showing some returns (via cost cutting).\\n\\nFinally, PIP, buyouts, and multiple smallish scale layoffs are the new norm to avoid negative headlines and pay less severance.', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Max95', 'name': 'Max95'}, 'upvoteCount': 4, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Layoffs are a feature, not a bug.', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Qué focal', 'name': 'Qué focal'}, 'upvoteCount': 2, 'commentCount': 0}, {'@type': 'Comment', 'text': 'No one&apos;s expecting a rate cut \\nhttps://www.cmegroup.com/markets/interest-rates/cme-fedwatch-tool.html', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Trump threatening jp', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'rokuukor', 'name': 'rokuukor'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'The day Trump strongarms Fed is the day the bond market will put trump back in his place.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'TravisHead', 'name': 'TravisHead'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'TravisHead', 'name': 'TravisHead'}, 'upvoteCount': 0, 'commentCount': 2}, {'@type': 'Comment', 'text': 'You mean you JUST had GIANT layoffs 2 weeks ago, people are just recovering and you want compounding fears?', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Kashicruch', 'name': 'Kashicruch'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 4}}\n",
      "  Scraping post 13/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'UK Laying of a Cancer patient while in treatment', 'text': 'A friend who works in UK and is going through active Cancer treatment has been just told that their role has been made redundant. The company is a large (A grade) investment banking organisation.\\n\\nThe reason given is reorg, cost cutting, role offshored etc.\\nThey are on a visa in UK, which gives them ability to stay in UK and healthcare. Without the job, they won&apos;t have visa and hence no access to healthcare.\\nIs there anything that can be done? Any advice would be much appreciated.', 'datePublished': '2025-06-15T14:00:00.000Z', 'url': 'https://www.teamblind.com/post/uk-laying-of-a-cancer-patient-while-in-treatment-znhssfj6', 'author': {'@type': 'Person', 'identifier': 'reliableme', 'name': 'reliableme'}, 'commentCount': 5, 'comment': [{'@type': 'Comment', 'text': 'As they won’t be paying tax in the UK they should not get free  healthcare as a non U.K. citizen - therefore they should go back to their original country or pay to go private, why should the U.K. tax payers fund them?', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'Every resident visa applicant in the UK pays ‘immigration health surcharge’ - that’s how the healthcare is funded for immigrants.', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': '🐓Hermanos', 'name': '🐓Hermanos'}, 'upvoteCount': 4, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'DQrh56', 'name': 'DQrh56'}, 'upvoteCount': 1, 'commentCount': 1}, {'@type': 'Comment', 'text': 'I don&apos;t think NHS differentiate whether your friend is on visa or native. I have been in UK for 23 years. Never been asked to see my visa documents', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'robLangdon', 'name': 'robLangdon'}, 'upvoteCount': 1, 'commentCount': 0}, {'@type': 'Comment', 'text': 'For UK layoff needs to go through extensive negotiation process with the staff representatives, it will not be immediate. There are chance that she can prolong her stay but it is not indefinite I am afraid!', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'gydgcshnk', 'name': 'gydgcshnk'}, 'upvoteCount': 1, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Is Cobra or an equivalent a thing there?\\n\\nCobra is a job loss solution to maintain your companies health care coverage for awhile.  It&apos;s obviously expensive but its specifically made for people in active treatment where you can&apos;t risk disruption of service or coverage of a specific provider that&apos;s helping you.\\n\\n(This is the specific use case, cobra isn&apos;t for just in case situations, use off the shelf self-insure for that)', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'rls3jd2sz', 'name': 'rls3jd2sz'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 1}}\n",
      "  Scraping post 14/22\n",
      "post_data: None\n",
      "  Scraping post 15/22\n",
      "post_data: None\n",
      "  Scraping post 16/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Amazon: Delay Pip with case against manager', 'text': 'Amazon folks, does opening a legitimate discrimination  case against your manager delay the Pip window until the case is resolved? Asking for a friend.\\n\\n#layoff #amazon', 'datePublished': '2025-06-14T18:00:00.000Z', 'url': 'https://www.teamblind.com/post/amazon-delay-pip-with-case-against-manager-dmxbnbed', 'author': {'@type': 'Person', 'identifier': 'uop456', 'name': 'uop456'}, 'commentCount': 5, 'comment': [{'@type': 'Comment', 'text': 'It may or may not, but we all wanna see how it goes. Please tell your friend to proceed and keep us posted.', 'datePublished': '2025-06-14T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'hail_vnch', 'name': 'hail_vnch'}, 'upvoteCount': 4, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Yes it does. It is paused until the investigation is over. Any progress made towards your PIP during this would be construed as retaliation.\\n\\nJust highlight this to your HR (through an email) and they will have the necessary conversations with the manager.', 'datePublished': '2025-06-14T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'But it guarantees termination no matter of the actual performance if claim is found bogus…', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'mynamalg', 'name': 'mynamalg'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'I mean it was going to happen anyway', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'Cisco📉', 'name': 'Cisco📉'}, 'upvoteCount': 1, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'Z0N', 'name': 'Z0N'}, 'upvoteCount': 2, 'commentCount': 2}, {'@type': 'Comment', 'text': 'Every single bogus claimant believes that their claim is “legitimate”', 'datePublished': '2025-06-14T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'mynamalg', 'name': 'mynamalg'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 2}}\n",
      "  Scraping post 17/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Urgently seeking referrals for SDE/Architect roles for US-based companies', 'text': 'Hi Dear friends,\\nI&apos;m a Software Engineer with 17+ years of experience with computer science degree in bachelors, currently on H1b with a  month left to secure a new role before my work authorization expires. I&apos;m actively looking for full-time opportunities in Software Engineering or Architect roles.\\n\\nIf you or your company are hiring, or if you’re open to referring me, I’d truly appreciate the help. I&apos;m happy to share my resume and any additional details via DM.\\n\\nThanks in advance to anyone who can support.', 'datePublished': '2025-06-14T18:00:00.000Z', 'url': 'https://www.teamblind.com/post/urgently-seeking-referrals-for-sdearchitect-roles-for-us-based-companies-nviznk0l', 'author': {'@type': 'Person', 'identifier': 'addy_m', 'name': 'addy_m'}, 'commentCount': 2, 'comment': [{'@type': 'Comment', 'text': 'If you’re interested in Zillow or find a role you’d want to apply for, feel free to DM.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'nepobby ', 'name': 'nepobby '}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'DM for Zillow if you find something you’d want a referral for.', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'nepobby ', 'name': 'nepobby '}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 4}}\n",
      "  Scraping post 18/22\n",
      "post_data: None\n",
      "  Scraping post 19/22\n",
      "post_data: None\n",
      "  Scraping post 20/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Friend pipped in MSFT', 'text': 'A friend got pipped in Microsoft recently. They are in security org. How bad are pips in Microsoft? Is it possible to complete the project given to complete in 2 months? Or is it just like Amazon where they eventually want to get rid of the employee?\\n\\n#layoff', 'datePublished': '2025-06-13T18:00:00.000Z', 'url': 'https://www.teamblind.com/post/friend-pipped-in-msft-moz0gmop', 'author': {'@type': 'Person', 'identifier': 'happydev', 'name': 'happydev'}, 'commentCount': 12, 'comment': [{'@type': 'Comment', 'text': 'What’s the gender of your friend who got pipped?\\n\\nOften it’s women, since their income is typically considered secondary in many cases.', 'datePublished': '2025-06-13T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'What??', 'datePublished': '2025-06-14T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'xYHXz', 'name': 'xYHXz'}, 'upvoteCount': 2, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Must be indian', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'hi7gho', 'name': 'hi7gho'}, 'upvoteCount': 9, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'ekKBry', 'name': 'ekKBry'}, 'upvoteCount': 2, 'commentCount': 2}, {'@type': 'Comment', 'text': 'Pips in general are meant to get rid of employees not give them a chance to prove themselves.', 'datePublished': '2025-06-13T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'SkGO37', 'name': 'SkGO37'}, 'upvoteCount': 11, 'commentCount': 0}, {'@type': 'Comment', 'text': 'I don’t know of people surviving. Just take gvsa.', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'yQpo77', 'name': 'yQpo77'}, 'upvoteCount': 1, 'commentCount': 0}, {'@type': 'Comment', 'text': 'You get a pip, take it as a heads up you’re about to get laid off and start working on your resume.', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'abodimw', 'name': 'abodimw'}, 'upvoteCount': 1, 'commentCount': 0}, {'@type': 'Comment', 'text': 'I think they’re prepping for July layoffs', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'RobotGlue', 'name': 'RobotGlue'}, 'upvoteCount': 1, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Nowhere near as bad as Amazon. Generally if you get PIP here you really have been slacking or are just not good at your job.', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'This was until like 2022.. Microsoft is cosplaying Amazon now except there’s no defined process and you get 🥜. If you get LITE, you could be laid off as soon as a month.', 'datePublished': '2025-06-16T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'dJDM32', 'name': 'dJDM32'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'Ac0p1L0t', 'name': 'Ac0p1L0t'}, 'upvoteCount': 0, 'commentCount': 1}, {'@type': 'Comment', 'text': 'Pips are a one way street', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'DanFodio', 'name': 'DanFodio'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': 'Its better to start looking elsewhere.', 'datePublished': '2025-06-15T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'salary_emp', 'name': 'salary_emp'}, 'upvoteCount': 0, 'commentCount': 0}, {'@type': 'Comment', 'text': '$0 severance given every month.', 'datePublished': '2025-06-14T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'uqiF88', 'name': 'uqiF88'}, 'upvoteCount': 0, 'commentCount': 0}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 5}}\n",
      "  Scraping post 21/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Help us sign the petition asking amazon to reform policies', 'text': 'We are asking you all for support in signing a petition that asks amazon to change and fix unfair policies affecting those with disabilities. Further, Amazon has violated NLRA collective bargaining rules by removing posts and discussions among employees related to these issues. \\nHere is the link to the petition \\nhttps://www.change.org/p/amazon-reform-policies-that-discriminate-against-those-with-disabilities-and-violate-ada\\n\\nhere is an articles in Bloomberg Published about this effort\\nhttps://www.bloomberg.com/news/articles/2025-06-12/amazon-s-return-to-office-mandate-sparks-disability-complaints?embedded-checkout=true\\n\\nYou can use an alias or pseudonym when signing to maintain anonymity', 'datePublished': '2025-06-12T23:00:00.000Z', 'url': 'https://www.teamblind.com/post/help-us-sign-the-petition-asking-amazon-to-reform-policies-cuzu7ij0', 'author': {'@type': 'Person', 'identifier': 'speakup1', 'name': 'speakup1'}, 'commentCount': 0, 'comment': [], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 1}}\n",
      "  Scraping post 22/22\n",
      "post_data: {'@context': 'https://schema.org', '@type': 'DiscussionForumPosting', 'headline': 'Best way to find a job after layoff', 'text': 'As the job market is tightening up with mid-year performance management coming up, more layoffs and hiring freezes, I wanted to ask people about what is the most effective way to find the next job.\\n\\n1. I have heard people stating that they are upfront about their layoffs, while others have not been open about it. I have heard people mentioning that they were ghosted by the recruiters after they mentioned being laid off.\\n2. Some people are settling for a lower position after being laid off and others are holding strong with their positions.\\n3. Many are applying for lesser total comp while others are not.\\n4. Some people are ready to move to any location in the country and others are wanting to stay remote.\\n\\nHow are people handling these decisions? Also, what strategies are people using to find their next jobs? I would love to hear what has worked for others.', 'datePublished': '2025-06-12T18:00:00.000Z', 'url': 'https://www.teamblind.com/post/best-way-to-find-a-job-after-layoff-nvlkvocy', 'author': {'@type': 'Person', 'identifier': 'Twincreek', 'name': 'Twincreek'}, 'commentCount': 7, 'comment': [{'@type': 'Comment', 'text': 'You write like Dr. Seuss', 'datePublished': '2025-06-12T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': '🕵️❌🖕w/Yu', 'name': '🕵️❌🖕w/Yu'}, 'upvoteCount': 13, 'commentCount': 0}, {'@type': 'Comment', 'text': 'If you’re interested in Zillow or find a role you’d want to apply for, feel free to DM.', 'datePublished': '2025-06-13T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'nepobby ', 'name': 'nepobby '}, 'upvoteCount': 3, 'commentCount': 0}, {'@type': 'Comment', 'text': 'There is a company which applies jobs on your behalf. Might be beneficial for those going through tough time, layoffs and time crunch. If this is helpful, i can share their details.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'What’s the company?', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'abeshinzo', 'name': 'abeshinzo'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'fNOC31', 'name': 'fNOC31'}, 'upvoteCount': 0, 'commentCount': 1}, {'@type': 'Comment', 'text': 'Consider my placement services where the payment is taken only after selection i.e. 10% of the annual comp.', 'datePublished': '2025-06-13T00:00:00.000Z', 'comment': [{'@type': 'Comment', 'text': 'How about 10% of this nuts.', 'datePublished': '2025-06-17T00:00:00.000Z', 'comment': [], 'author': {'@type': 'Person', 'identifier': 'abeshinzo', 'name': 'abeshinzo'}, 'upvoteCount': 0, 'commentCount': 0}], 'author': {'@type': 'Person', 'identifier': 'xhBX36', 'name': 'xhBX36'}, 'upvoteCount': 0, 'commentCount': 1}], 'interactionStatistic': {'@type': 'InteractionCounter', 'interactionType': {'@type': 'LikeAction'}, 'userInteractionCount': 5}}\n",
      "No more pages\n",
      "Saved 15 posts to layoffs_posts.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime,date\n",
    "import json\n",
    "from urllib.parse import urljoin, urlencode\n",
    "import time\n",
    "import re\n",
    "\n",
    "MIN_DATE = date(2025, 1, 1)  # Use simple date object (no time)\n",
    "BASE_URL = \"https://www.teamblind.com\"\n",
    "TOPIC_URL = f\"{BASE_URL}/topics/General-Topics/Layoffs\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "def extract_json_ld(html):\n",
    "    \"\"\"Extract JSON-LD data from post page\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    script = soup.find('script', {'id': 'article-discussion-forum-posting-schema', 'type': 'application/ld+json'})\n",
    "    \n",
    "    if not script:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        return json.loads(script.string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def scrape_post(url):\n",
    "    \"\"\"Scrape individual post page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        post_data = extract_json_ld(response.text)\n",
    "        print(f\"post_data: {post_data}\")\n",
    "        if not post_data:\n",
    "            return None\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        # Convert to date object\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        if post_date <= MIN_DATE:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_data[\"datePublished\"],\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_topic_page(url):\n",
    "    \"\"\"Scrape a single page of the topic\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Extract post links\n",
    "        post_links = []\n",
    "        for article in soup.select('article[data-testid=\"article-preview-card\"]'):\n",
    "            link = article.select_one('a[data-testid=\"article-preview-click-box\"]')\n",
    "            if link and link.get('href'):\n",
    "                post_links.append(urljoin(BASE_URL, link['href']))\n",
    "        \n",
    "        # Find next page token\n",
    "        next_token = None\n",
    "        load_more = soup.select_one('a[data-testid=\"load-more-button\"]')\n",
    "        if load_more:\n",
    "            href = load_more.get('href', '')\n",
    "            token_match = re.search(r'pageToken=([^&]+)', href)\n",
    "            if token_match:\n",
    "                next_token = token_match.group(1)\n",
    "        \n",
    "        return post_links, next_token\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping topic page: {str(e)}\")\n",
    "        return [], None\n",
    "\n",
    "def scrape_layoffs():\n",
    "    \"\"\"Main scraping function\"\"\"\n",
    "    all_posts = []\n",
    "    next_token = None\n",
    "    page = 1\n",
    "    max_pages = 50  # Safety limit\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        \n",
    "        # Build URL with pagination parameters\n",
    "        if next_token:\n",
    "            params = {\"page\": page, \"pageToken\": next_token}\n",
    "            url = f\"{TOPIC_URL}?{urlencode(params)}\"\n",
    "        else:\n",
    "            url = TOPIC_URL\n",
    "            \n",
    "        post_links, next_token = scrape_topic_page(url)\n",
    "        \n",
    "        if not post_links:\n",
    "            print(\"No posts found on page\")\n",
    "            break\n",
    "            \n",
    "        # Process posts with rate limiting\n",
    "        for i, post_url in enumerate(post_links):\n",
    "            print(f\"  Scraping post {i+1}/{len(post_links)}\")\n",
    "            post_data = scrape_post(post_url)\n",
    "            if post_data:\n",
    "                all_posts.append(post_data)\n",
    "            time.sleep(1.5)  # Respectful delay\n",
    "            \n",
    "        if not next_token:\n",
    "            print(\"No more pages\")\n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "        time.sleep(2)  # Delay between pages\n",
    "    \n",
    "    return all_posts\n",
    "\n",
    "def save_results(posts):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    with open('layoffs_posts.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(posts)} posts to layoffs_posts.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    posts = scrape_layoffs()\n",
    "    save_results(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN_DATE: 2025-01-01\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "MIN_DATE = date(2025, 1, 1)\n",
    "\n",
    "print(f\"MIN_DATE: {MIN_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting urllib3~=2.4.0 (from urllib3[socks]~=2.4.0->selenium)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.4.26 (from selenium)\n",
      "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting typing_extensions~=4.13.2 (from selenium)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.30.0->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/csgrads/shahr072/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n",
      "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, urllib3, typing_extensions, certifi, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.3.0 certifi-2025.6.15 outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 urllib3-2.4.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handled infinite scroll but can't seem to stop scrolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page and simulating infinite scroll...\n",
      "Scrolled to bottom, new height: 11796\n",
      "Scrolled to bottom, new height: 16975\n",
      "Scrolled to bottom, new height: 23037\n",
      "Scrolled to bottom, new height: 28745\n",
      "Scrolled to bottom, new height: 34125\n",
      "Scrolled to bottom, new height: 40122\n",
      "Scrolled to bottom, new height: 40122\n",
      "Scrolled to bottom, new height: 40122\n",
      "Scrolled to bottom, new height: 46782\n",
      "Scrolled to bottom, new height: 52382\n",
      "Scrolled to bottom, new height: 57892\n",
      "Scrolled to bottom, new height: 64424\n",
      "Scrolled to bottom, new height: 69240\n",
      "Scrolled to bottom, new height: 74269\n",
      "Scrolled to bottom, new height: 80355\n",
      "Scrolled to bottom, new height: 85374\n",
      "Scrolled to bottom, new height: 91610\n",
      "Scrolled to bottom, new height: 91610\n",
      "Scrolled to bottom, new height: 98259\n",
      "Scrolled to bottom, new height: 104212\n",
      "Scrolled to bottom, new height: 108906\n",
      "Scrolled to bottom, new height: 115100\n",
      "Scrolled to bottom, new height: 120556\n",
      "Scrolled to bottom, new height: 126752\n",
      "Scrolled to bottom, new height: 132201\n",
      "Scrolled to bottom, new height: 136995\n",
      "Scrolled to bottom, new height: 142246\n",
      "Scrolled to bottom, new height: 148178\n",
      "Scrolled to bottom, new height: 154684\n",
      "Scrolled to bottom, new height: 154684\n",
      "Scrolled to bottom, new height: 161298\n",
      "Scrolled to bottom, new height: 166842\n",
      "Scrolled to bottom, new height: 172469\n",
      "Scrolled to bottom, new height: 178427\n",
      "Scrolled to bottom, new height: 184485\n",
      "Scrolled to bottom, new height: 190406\n",
      "Scrolled to bottom, new height: 195775\n",
      "Scrolled to bottom, new height: 201261\n",
      "Scrolled to bottom, new height: 201261\n",
      "Scrolled to bottom, new height: 207073\n",
      "Scrolled to bottom, new height: 212695\n",
      "Scrolled to bottom, new height: 218379\n",
      "Scrolled to bottom, new height: 224210\n",
      "Scrolled to bottom, new height: 229878\n",
      "Scrolled to bottom, new height: 229878\n",
      "Scrolled to bottom, new height: 234722\n",
      "Scrolled to bottom, new height: 240580\n",
      "Scrolled to bottom, new height: 245436\n",
      "Scrolled to bottom, new height: 245436\n",
      "Scrolled to bottom, new height: 250882\n",
      "Scrolled to bottom, new height: 256480\n",
      "Scrolled to bottom, new height: 256480\n",
      "Scrolled to bottom, new height: 261868\n",
      "Scrolled to bottom, new height: 266994\n",
      "Scrolled to bottom, new height: 266994\n",
      "Scrolled to bottom, new height: 273093\n",
      "Scrolled to bottom, new height: 273093\n",
      "Scrolled to bottom, new height: 278262\n",
      "Scrolled to bottom, new height: 278262\n",
      "Scrolled to bottom, new height: 284429\n",
      "Scrolled to bottom, new height: 284429\n",
      "Scrolled to bottom, new height: 289952\n",
      "Scrolled to bottom, new height: 295098\n",
      "Scrolled to bottom, new height: 295098\n",
      "Scrolled to bottom, new height: 300268\n",
      "Scrolled to bottom, new height: 300268\n",
      "Scrolled to bottom, new height: 305880\n",
      "Scrolled to bottom, new height: 305880\n",
      "Scrolled to bottom, new height: 311921\n",
      "Scrolled to bottom, new height: 311921\n",
      "Scrolled to bottom, new height: 317939\n",
      "Scrolled to bottom, new height: 317939\n",
      "Scrolled to bottom, new height: 322729\n",
      "Scrolled to bottom, new height: 322729\n",
      "Scrolled to bottom, new height: 328009\n",
      "Scrolled to bottom, new height: 328009\n",
      "Scrolled to bottom, new height: 333234\n",
      "Scrolled to bottom, new height: 333234\n",
      "Scrolled to bottom, new height: 338598\n",
      "Scrolled to bottom, new height: 338598\n",
      "Scrolled to bottom, new height: 343407\n",
      "Scrolled to bottom, new height: 343407\n",
      "Scrolled to bottom, new height: 348691\n",
      "Scrolled to bottom, new height: 348691\n",
      "Scrolled to bottom, new height: 349980\n",
      "Scrolled to bottom, new height: 349980\n",
      "Scrolled to bottom, new height: 355734\n",
      "Scrolled to bottom, new height: 355734\n",
      "Scrolled to bottom, new height: 360890\n",
      "Scrolled to bottom, new height: 360890\n",
      "Scrolled to bottom, new height: 366290\n",
      "Scrolled to bottom, new height: 366290\n",
      "Scrolled to bottom, new height: 371486\n",
      "Scrolled to bottom, new height: 371486\n",
      "Scrolled to bottom, new height: 376924\n",
      "Scrolled to bottom, new height: 376924\n",
      "Scrolled to bottom, new height: 382852\n",
      "Scrolled to bottom, new height: 382852\n",
      "Scrolled to bottom, new height: 387720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts to layoffs_posts.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     posts \u001b[38;5;241m=\u001b[39m scrape_layoffs()\n\u001b[1;32m    169\u001b[0m     save_results(posts)\n",
      "Cell \u001b[0;32mIn[11], line 139\u001b[0m, in \u001b[0;36mscrape_layoffs\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m driver \u001b[38;5;241m=\u001b[39m setup_driver()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Get all post links through infinite scroll\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     post_links \u001b[38;5;241m=\u001b[39m get_all_post_links(driver)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m post_links:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo posts found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 100\u001b[0m, in \u001b[0;36mget_all_post_links\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m     97\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Allow content to load\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Check if we've reached the end\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m new_height \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn document.body.scrollHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_height \u001b[38;5;241m==\u001b[39m last_height:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Check for \"no more posts\" message\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     end_message \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//*[contains(text(), \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo more posts to load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:544\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    541\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[1;32m    542\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(command, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m: script, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: converted_args})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    443\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 445\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m    145\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m    787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[0;32m--> 789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Set properties that are used by the pooling layer.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m # Since the connection's timeout value may have been updated\n\u001b[1;32m    504\u001b[0m # we need to set the timeout on the socket.\n\u001b[1;32m    505\u001b[0m self.sock.settimeout(self.timeout)\n\u001b[0;32m--> 507\u001b[0m # This is needed here to avoid circular import errors\n\u001b[1;32m    508\u001b[0m from .response import HTTPResponse\n\u001b[1;32m    510\u001b[0m # Save a reference to the shutdown function before ownership is passed\n\u001b[1;32m    511\u001b[0m # to httplib_response\n\u001b[1;32m    512\u001b[0m # TODO should we implement it everywhere?\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "MIN_DATE = date(2025, 1, 1)  # Use simple date object (no time)\n",
    "BASE_URL = \"https://www.teamblind.com\"\n",
    "TOPIC_URL = f\"{BASE_URL}/topics/General-Topics/Layoffs\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# Configure Selenium\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,800\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Network.setUserAgentOverride\",\n",
    "        {\"userAgent\": HEADERS[\"User-Agent\"]}\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def extract_json_ld(html):\n",
    "    \"\"\"Extract JSON-LD data from post page\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    script = soup.find('script', {'id': 'article-discussion-forum-posting-schema', 'type': 'application/ld+json'})\n",
    "    \n",
    "    if not script:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        return json.loads(script.string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def scrape_post(url):\n",
    "    \"\"\"Scrape individual post page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        post_data = extract_json_ld(response.text)\n",
    "        if not post_data:\n",
    "            return None\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        # Convert to date object\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        if post_date <= MIN_DATE:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_data[\"datePublished\"],\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_all_post_links(driver):\n",
    "    \"\"\"Get all post links by simulating infinite scroll\"\"\"\n",
    "    print(\"Loading page and simulating infinite scroll...\")\n",
    "    driver.get(TOPIC_URL)\n",
    "    \n",
    "    # Wait for initial content to load\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "    )\n",
    "    \n",
    "    # Scroll until no more content loads\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_attempts = 0\n",
    "    max_attempts = 20  # Safety limit\n",
    "    \n",
    "    while scroll_attempts < max_attempts:\n",
    "        # Scroll to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow content to load\n",
    "        \n",
    "        # Check if we've reached the end\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # Check for \"no more posts\" message\n",
    "            end_message = driver.find_elements(By.XPATH, \"//*[contains(text(), 'No more posts to load')]\")\n",
    "            if end_message:\n",
    "                print(\"Reached end of content\")\n",
    "                break\n",
    "            scroll_attempts += 1\n",
    "        else:\n",
    "            scroll_attempts = 0  # Reset counter if we got new content\n",
    "        \n",
    "        last_height = new_height\n",
    "        print(f\"Scrolled to bottom, new height: {new_height}\")\n",
    "    \n",
    "    # Extract all post links\n",
    "    post_links = []\n",
    "    articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]')\n",
    "    print(f\"Found {len(articles)} posts\")\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            link = article.find_element(By.CSS_SELECTOR, 'a[data-testid=\"article-preview-click-box\"]')\n",
    "            href = link.get_attribute('href')\n",
    "            if href:\n",
    "                post_links.append(href)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return post_links\n",
    "\n",
    "def scrape_layoffs():\n",
    "    \"\"\"Main scraping function\"\"\"\n",
    "    all_posts = []\n",
    "    \n",
    "    # Set up Selenium driver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Get all post links through infinite scroll\n",
    "        post_links = get_all_post_links(driver)\n",
    "        \n",
    "        if not post_links:\n",
    "            print(\"No posts found\")\n",
    "            return all_posts\n",
    "        \n",
    "        print(f\"Found {len(post_links)} posts. Starting to scrape individual posts...\")\n",
    "        \n",
    "        # Process posts with rate limiting\n",
    "        for i, post_url in enumerate(post_links):\n",
    "            print(f\"  Scraping post {i+1}/{len(post_links)}: {post_url}\")\n",
    "            post_data = scrape_post(post_url)\n",
    "            if post_data:\n",
    "                all_posts.append(post_data)\n",
    "            time.sleep(1.5)  # Respectful delay\n",
    "    \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_posts\n",
    "\n",
    "def save_results(posts):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    with open('teamblind_layoffs_posts.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(posts)} posts to layoffs_posts.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    posts = scrape_layoffs()\n",
    "    save_results(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to track with post count instead of load more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page and simulating infinite scroll...\n",
      "Current post count: 42\n",
      "Current post count: 62\n",
      "Current post count: 82\n",
      "Current post count: 102\n",
      "Current post count: 122\n",
      "Current post count: 142\n",
      "Current post count: 162\n",
      "Current post count: 182\n",
      "Current post count: 202\n",
      "Current post count: 222\n",
      "Current post count: 242\n",
      "Current post count: 262\n",
      "Current post count: 282\n",
      "Current post count: 302\n",
      "Current post count: 322\n",
      "Current post count: 342\n",
      "Current post count: 362\n",
      "Current post count: 382\n",
      "Current post count: 402\n",
      "Current post count: 422\n",
      "Current post count: 442\n",
      "Current post count: 462\n",
      "Current post count: 482\n",
      "Current post count: 502\n",
      "Current post count: 522\n",
      "Current post count: 542\n",
      "Current post count: 562\n",
      "Current post count: 582\n",
      "Current post count: 602\n",
      "Current post count: 622\n",
      "Current post count: 642\n",
      "Current post count: 662\n",
      "Current post count: 682\n",
      "Current post count: 702\n",
      "Current post count: 722\n",
      "Current post count: 742\n",
      "Current post count: 762\n",
      "Current post count: 782\n",
      "Current post count: 802\n",
      "Current post count: 822\n",
      "Current post count: 842\n",
      "Current post count: 862\n",
      "Current post count: 882\n",
      "Current post count: 902\n",
      "Current post count: 922\n",
      "Current post count: 942\n",
      "Current post count: 962\n",
      "Current post count: 982\n",
      "Current post count: 1002\n",
      "Current post count: 1022\n",
      "Current post count: 1042\n",
      "Current post count: 1062\n",
      "Current post count: 1082\n",
      "Current post count: 1102\n",
      "Current post count: 1122\n",
      "Current post count: 1142\n",
      "Current post count: 1162\n",
      "Current post count: 1182\n",
      "Current post count: 1182\n",
      "No new posts (1/15)\n",
      "Current post count: 1202\n",
      "Current post count: 1222\n",
      "Current post count: 1222\n",
      "No new posts (1/15)\n",
      "Current post count: 1242\n",
      "Current post count: 1262\n",
      "Current post count: 1282\n",
      "Current post count: 1302\n",
      "Current post count: 1302\n",
      "No new posts (1/15)\n",
      "Current post count: 1322\n",
      "Current post count: 1342\n",
      "Current post count: 1362\n",
      "Current post count: 1362\n",
      "No new posts (1/15)\n",
      "Current post count: 1382\n",
      "Current post count: 1402\n",
      "Current post count: 1422\n",
      "Current post count: 1422\n",
      "No new posts (1/15)\n",
      "Current post count: 1442\n",
      "Current post count: 1442\n",
      "No new posts (1/15)\n",
      "Current post count: 1462\n",
      "Current post count: 1482\n",
      "Current post count: 1482\n",
      "No new posts (1/15)\n",
      "Current post count: 1502\n",
      "Current post count: 1522\n",
      "Current post count: 1522\n",
      "No new posts (1/15)\n",
      "Current post count: 1542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts to layoffs_posts.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 172\u001b[0m     posts \u001b[38;5;241m=\u001b[39m scrape_layoffs()\n\u001b[1;32m    173\u001b[0m     save_results(posts)\n",
      "Cell \u001b[0;32mIn[12], line 143\u001b[0m, in \u001b[0;36mscrape_layoffs\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m driver \u001b[38;5;241m=\u001b[39m setup_driver()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# Get all post links through infinite scroll\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     post_links \u001b[38;5;241m=\u001b[39m get_all_post_links(driver)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m post_links:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo posts found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 98\u001b[0m, in \u001b[0;36mget_all_post_links\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m scroll_attempts \u001b[38;5;241m<\u001b[39m max_attempts:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Scroll to bottom\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2.5\u001b[39m)  \u001b[38;5;66;03m# Allow time for content to load\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Get current post count\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     current_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle[data-testid=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle-preview-card\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "MIN_DATE = date(2025, 1, 1)  # Use simple date object (no time)\n",
    "BASE_URL = \"https://www.teamblind.com\"\n",
    "TOPIC_URL = f\"{BASE_URL}/topics/General-Topics/Layoffs\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# Configure Selenium\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,800\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Network.setUserAgentOverride\",\n",
    "        {\"userAgent\": HEADERS[\"User-Agent\"]}\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def extract_json_ld(html):\n",
    "    \"\"\"Extract JSON-LD data from post page\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    script = soup.find('script', {'id': 'article-discussion-forum-posting-schema', 'type': 'application/ld+json'})\n",
    "    \n",
    "    if not script:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        return json.loads(script.string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def scrape_post(url):\n",
    "    \"\"\"Scrape individual post page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        post_data = extract_json_ld(response.text)\n",
    "        if not post_data:\n",
    "            return None\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        # Convert to date object\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        if post_date <= MIN_DATE:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_data[\"datePublished\"],\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_all_post_links(driver):\n",
    "    \"\"\"Get all post links by simulating infinite scroll\"\"\"\n",
    "    print(\"Loading page and simulating infinite scroll...\")\n",
    "    driver.get(TOPIC_URL)\n",
    "    \n",
    "    # Wait for initial content\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "    )\n",
    "    \n",
    "    # Track post counts to detect when loading stops\n",
    "    prev_count = 0\n",
    "    current_count = len(driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "    scroll_attempts = 0\n",
    "    max_attempts = 15  # Safety limit\n",
    "    \n",
    "    while scroll_attempts < max_attempts:\n",
    "        # Scroll to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2.5)  # Allow time for content to load\n",
    "        \n",
    "        # Get current post count\n",
    "        current_count = len(driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "        print(f\"Current post count: {current_count}\")\n",
    "        \n",
    "        # Check if new posts were loaded\n",
    "        if current_count == prev_count:\n",
    "            scroll_attempts += 1\n",
    "            print(f\"No new posts ({scroll_attempts}/{max_attempts})\")\n",
    "        else:\n",
    "            scroll_attempts = 0  # Reset counter if new posts loaded\n",
    "            prev_count = current_count\n",
    "        \n",
    "        # Check for end of content message\n",
    "        end_messages = driver.find_elements(By.XPATH, \"//*[contains(., 'No more posts to load')]\")\n",
    "        if end_messages:\n",
    "            print(\"Detected end of content message\")\n",
    "            break\n",
    "    \n",
    "    # Extract all post links\n",
    "    post_links = []\n",
    "    articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]')\n",
    "    print(f\"Finished scrolling. Total posts: {len(articles)}\")\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            link = article.find_element(By.CSS_SELECTOR, 'a[data-testid=\"article-preview-click-box\"]')\n",
    "            href = link.get_attribute('href')\n",
    "            if href:\n",
    "                post_links.append(href)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return post_links\n",
    "\n",
    "def scrape_layoffs():\n",
    "    \"\"\"Main scraping function\"\"\"\n",
    "    all_posts = []\n",
    "    \n",
    "    # Set up Selenium driver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Get all post links through infinite scroll\n",
    "        post_links = get_all_post_links(driver)\n",
    "        \n",
    "        if not post_links:\n",
    "            print(\"No posts found\")\n",
    "            return all_posts\n",
    "        \n",
    "        print(f\"Found {len(post_links)} posts. Starting to scrape individual posts...\")\n",
    "        \n",
    "        # Process posts with rate limiting\n",
    "        for i, post_url in enumerate(post_links):\n",
    "            print(f\"  Scraping post {i+1}/{len(post_links)}: {post_url}\")\n",
    "            post_data = scrape_post(post_url)\n",
    "            if post_data:\n",
    "                all_posts.append(post_data)\n",
    "            time.sleep(1.5)  # Respectful delay\n",
    "    \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_posts\n",
    "\n",
    "def save_results(posts):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    with open('teamblind_layoffs_posts.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(posts)} posts to layoffs_posts.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    posts = scrape_layoffs()\n",
    "    save_results(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial page...\n",
      "Processing 22 new posts...\n",
      "  Scraping post 1/22\n",
      "  Scraping post 2/22\n",
      "  Scraping post 3/22\n",
      "  Scraping post 4/22\n",
      "  Scraping post 5/22\n",
      "  Scraping post 6/22\n",
      "  Scraping post 7/22\n",
      "  Scraping post 8/22\n",
      "  Scraping post 9/22\n",
      "  Scraping post 10/22\n",
      "  Scraping post 11/22\n",
      "  Scraping post 12/22\n",
      "  Scraping post 13/22\n",
      "  Scraping post 14/22\n",
      "  Scraping post 15/22\n",
      "  Scraping post 16/22\n",
      "  Scraping post 17/22\n",
      "  Scraping post 18/22\n",
      "  Scraping post 19/22\n",
      "  Scraping post 20/22\n",
      "  Scraping post 21/22\n",
      "  Scraping post 22/22\n",
      "Scrolling to bottom (attempt 1/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 42\n",
      "Scrolling to bottom (attempt 2/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 62\n",
      "Scrolling to bottom (attempt 3/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 82\n",
      "Scrolling to bottom (attempt 4/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 102\n",
      "Scrolling to bottom (attempt 5/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 122\n",
      "Scrolling to bottom (attempt 6/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 142\n",
      "Scrolling to bottom (attempt 7/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 162\n",
      "Scrolling to bottom (attempt 8/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 182\n",
      "Scrolling to bottom (attempt 9/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 202\n",
      "Scrolling to bottom (attempt 10/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 222\n",
      "Scrolling to bottom (attempt 11/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 242\n",
      "Scrolling to bottom (attempt 12/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 262\n",
      "Scrolling to bottom (attempt 13/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 282\n",
      "Scrolling to bottom (attempt 14/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 302\n",
      "Scrolling to bottom (attempt 15/15)\n",
      "Processing 20 new posts...\n",
      "  Scraping post 1/20\n",
      "  Scraping post 2/20\n",
      "  Scraping post 3/20\n",
      "  Scraping post 4/20\n",
      "  Scraping post 5/20\n",
      "  Scraping post 6/20\n",
      "  Scraping post 7/20\n",
      "  Scraping post 8/20\n",
      "  Scraping post 9/20\n",
      "  Scraping post 10/20\n",
      "  Scraping post 11/20\n",
      "  Scraping post 12/20\n",
      "  Scraping post 13/20\n",
      "  Scraping post 14/20\n",
      "  Scraping post 15/20\n",
      "  Scraping post 16/20\n",
      "  Scraping post 17/20\n",
      "  Scraping post 18/20\n",
      "  Scraping post 19/20\n",
      "  Scraping post 20/20\n",
      "Total posts processed: 322\n",
      "Saved 246 posts to layoffs_posts.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "MIN_DATE = date(2025, 1, 1)  # Use simple date object (no time)\n",
    "BASE_URL = \"https://www.teamblind.com\"\n",
    "TOPIC_URL = f\"{BASE_URL}/topics/General-Topics/Layoffs\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# Configure Selenium\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,800\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Network.setUserAgentOverride\",\n",
    "        {\"userAgent\": HEADERS[\"User-Agent\"]}\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def extract_json_ld(html):\n",
    "    \"\"\"Extract JSON-LD data from post page\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    script = soup.find('script', {'id': 'article-discussion-forum-posting-schema', 'type': 'application/ld+json'})\n",
    "    \n",
    "    if not script:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        return json.loads(script.string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def scrape_post(url):\n",
    "    \"\"\"Scrape individual post page and return (data, is_old) tuple\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        post_data = extract_json_ld(response.text)\n",
    "        if not post_data:\n",
    "            return None, False\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        # Convert to date object\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        \n",
    "        if post_date <= MIN_DATE:\n",
    "            return None, True  # Post is too old\n",
    "            \n",
    "        return {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_data[\"datePublished\"],\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"]\n",
    "        }, False\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None, False\n",
    "\n",
    "def process_new_posts(driver, processed_links, all_posts):\n",
    "    \"\"\"Process newly loaded posts and return whether we should stop\"\"\"\n",
    "    # Get all current post elements\n",
    "    current_elements = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]')\n",
    "    \n",
    "    new_posts = []\n",
    "    for element in current_elements:\n",
    "        try:\n",
    "            link = element.find_element(By.CSS_SELECTOR, 'a[data-testid=\"article-preview-click-box\"]')\n",
    "            href = link.get_attribute('href')\n",
    "            if href and href not in processed_links:\n",
    "                new_posts.append(href)\n",
    "                processed_links.add(href)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not new_posts:\n",
    "        return False  # No new posts found, don't stop\n",
    "    \n",
    "    print(f\"Processing {len(new_posts)} new posts...\")\n",
    "    should_stop = False\n",
    "    \n",
    "    for i, post_url in enumerate(new_posts):\n",
    "        print(f\"  Scraping post {i+1}/{len(new_posts)}\")\n",
    "        post_data, is_old = scrape_post(post_url)\n",
    "        \n",
    "        if is_old:\n",
    "            print(f\"Found old post ({post_url}), stopping processing\")\n",
    "            should_stop = True\n",
    "            break\n",
    "            \n",
    "        if post_data:\n",
    "            all_posts.append(post_data)\n",
    "            # Save results after each post\n",
    "            save_results(all_posts)\n",
    "    \n",
    "    return should_stop\n",
    "\n",
    "def save_results(posts):\n",
    "    \"\"\"Save results to JSON file incrementally\"\"\"\n",
    "    with open('layoffs_posts.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def scrape_layoffs():\n",
    "    \"\"\"Main scraping function with incremental processing\"\"\"\n",
    "    all_posts = []\n",
    "    processed_links = set()\n",
    "    scroll_attempts = 0\n",
    "    max_attempts = 15  # Safety limit\n",
    "    \n",
    "    # Set up Selenium driver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading initial page...\")\n",
    "        driver.get(TOPIC_URL)\n",
    "        \n",
    "        # Wait for initial content\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "        )\n",
    "        \n",
    "        # Process initial batch of posts\n",
    "        if process_new_posts(driver, processed_links, all_posts):\n",
    "            print(\"Found old posts in initial batch, stopping\")\n",
    "            return all_posts\n",
    "        \n",
    "        # Scroll and process incrementally\n",
    "        while scroll_attempts < max_attempts:\n",
    "            print(f\"Scrolling to bottom (attempt {scroll_attempts+1}/{max_attempts})\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2.5)  # Allow content to load\n",
    "            \n",
    "            # Check for end of content message\n",
    "            end_messages = driver.find_elements(By.XPATH, \"//*[contains(., 'No more posts to load')]\")\n",
    "            if end_messages:\n",
    "                print(\"Detected end of content message\")\n",
    "                break\n",
    "            \n",
    "            # Process new posts\n",
    "            should_stop = process_new_posts(driver, processed_links, all_posts)\n",
    "            if should_stop:\n",
    "                print(\"Stopping due to old post found\")\n",
    "                break\n",
    "                \n",
    "            # Check if we found new posts\n",
    "            current_count = len(processed_links)\n",
    "            print(f\"Total posts processed: {current_count}\")\n",
    "            \n",
    "            if current_count == len(driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]')):\n",
    "                scroll_attempts += 1\n",
    "            else:\n",
    "                scroll_attempts = 0  # Reset if we found new posts\n",
    "    \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_posts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    posts = scrape_layoffs()\n",
    "    save_results(posts)\n",
    "    print(f\"Saved {len(posts)} posts to layoffs_posts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final version with only date comparison as breakpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial page...\n",
      "Processing 22 new posts...\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 29\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 44\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 62\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 78\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 94\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 108\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 124\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 142\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 157\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 173\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 187\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 200\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 216\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 231\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 246\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 261\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n",
      "Total posts scraped until now: 274\n",
      "Scrolling to bottom...\n",
      "Processing 20 new posts...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 187\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_posts, stop_reason\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     posts, reason \u001b[38;5;241m=\u001b[39m scrape_layoffs()\n\u001b[1;32m    188\u001b[0m     save_results(posts)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts to teamblind_layoffs_posts.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 162\u001b[0m, in \u001b[0;36mscrape_layoffs\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Process new posts\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m reason \u001b[38;5;241m=\u001b[39m process_new_posts(driver, processed_links, all_posts)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal posts scraped until now: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_post_found\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[18], line 101\u001b[0m, in \u001b[0;36mprocess_new_posts\u001b[0;34m(driver, processed_links, all_posts)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new posts...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, post_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_posts):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m#print(f\"  Scraping post {i+1}/{len(new_posts)}\")\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     post_data, is_old \u001b[38;5;241m=\u001b[39m scrape_post(post_url)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_old:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound old post (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), stopping processing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 52\u001b[0m, in \u001b[0;36mscrape_post\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Scrape individual post page and return (data, is_old) tuple\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mHEADERS, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     53\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     55\u001b[0m     post_data \u001b[38;5;241m=\u001b[39m extract_json_ld(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m    787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[0;32m--> 789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Set properties that are used by the pooling layer.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m # Since the connection's timeout value may have been updated\n\u001b[1;32m    504\u001b[0m # we need to set the timeout on the socket.\n\u001b[1;32m    505\u001b[0m self.sock.settimeout(self.timeout)\n\u001b[0;32m--> 507\u001b[0m # This is needed here to avoid circular import errors\n\u001b[1;32m    508\u001b[0m from .response import HTTPResponse\n\u001b[1;32m    510\u001b[0m # Save a reference to the shutdown function before ownership is passed\n\u001b[1;32m    511\u001b[0m # to httplib_response\n\u001b[1;32m    512\u001b[0m # TODO should we implement it everywhere?\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import json\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "MIN_DATE = date(2024, 1, 1)  # Use simple date object (no time)\n",
    "BASE_URL = \"https://www.teamblind.com\"\n",
    "TOPIC_URL = f\"{BASE_URL}/topics/General-Topics/Layoffs\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# Configure Selenium\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,800\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Network.setUserAgentOverride\",\n",
    "        {\"userAgent\": HEADERS[\"User-Agent\"]}\n",
    "    )\n",
    "    return driver\n",
    "\n",
    "def extract_json_ld(html):\n",
    "    \"\"\"Extract JSON-LD data from post page\"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    script = soup.find('script', {'id': 'article-discussion-forum-posting-schema', 'type': 'application/ld+json'})\n",
    "    \n",
    "    if not script:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        return json.loads(script.string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def scrape_post(url):\n",
    "    \"\"\"Scrape individual post page and return (data, is_old) tuple\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        post_data = extract_json_ld(response.text)\n",
    "        if not post_data:\n",
    "            return None, False\n",
    "            \n",
    "        # Extract just the date part (first 10 characters: YYYY-MM-DD)\n",
    "        post_date_str = post_data[\"datePublished\"][:10]\n",
    "        # Convert to date object\n",
    "        post_date = datetime.strptime(post_date_str, \"%Y-%m-%d\").date()\n",
    "        \n",
    "        if post_date <= MIN_DATE:\n",
    "            return None, True  # Post is too old\n",
    "            \n",
    "        return {\n",
    "            \"headline\": post_data[\"headline\"],\n",
    "            \"text\": post_data[\"text\"],\n",
    "            \"date\": post_date_str,  # Store only the date part\n",
    "            \"url\": post_data[\"url\"],\n",
    "            \"commentCount\": post_data[\"commentCount\"]\n",
    "        }, False\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None, False\n",
    "\n",
    "def process_new_posts(driver, processed_links, all_posts):\n",
    "    \"\"\"Process newly loaded posts and return stop reason (or None)\"\"\"\n",
    "    # Get all current post elements\n",
    "    current_elements = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]')\n",
    "    \n",
    "    new_posts = []\n",
    "    for element in current_elements:\n",
    "        try:\n",
    "            link = element.find_element(By.CSS_SELECTOR, 'a[data-testid=\"article-preview-click-box\"]')\n",
    "            href = link.get_attribute('href')\n",
    "            if href and href not in processed_links:\n",
    "                new_posts.append(href)\n",
    "                processed_links.add(href)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if not new_posts:\n",
    "        return \"no_new_posts\"  # No new posts found\n",
    "    \n",
    "    print(f\"Processing {len(new_posts)} new posts...\")\n",
    "    \n",
    "    for i, post_url in enumerate(new_posts):\n",
    "        #print(f\"  Scraping post {i+1}/{len(new_posts)}\")\n",
    "        post_data, is_old = scrape_post(post_url)\n",
    "        \n",
    "        if is_old:\n",
    "            print(f\"Found old post ({post_url}), stopping processing\")\n",
    "            return \"old_post_found\"\n",
    "            \n",
    "        if post_data:\n",
    "            all_posts.append(post_data)\n",
    "            if len(all_posts) % 1000 == 0:\n",
    "                last_post_date = all_posts[-1]['date'] if all_posts else \"N/A\"\n",
    "                print(f\"Total posts scraped until now: {len(all_posts)}. Last post date: {last_post_date}\")\n",
    "            # Save results after each post\n",
    "            save_results(all_posts)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def save_results(posts):\n",
    "    \"\"\"Save results to JSON file incrementally\"\"\"\n",
    "    with open('teamblind_layoffs_posts.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(posts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def scrape_layoffs():\n",
    "    \"\"\"Main scraping function with incremental processing\"\"\"\n",
    "    all_posts = []\n",
    "    processed_links = set()\n",
    "    consecutive_no_new = 0\n",
    "    max_consecutive_no_new = 15  # Safety limit for no new posts\n",
    "    \n",
    "    # Set up Selenium driver\n",
    "    driver = setup_driver()\n",
    "    stop_reason = None\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading initial page...\")\n",
    "        driver.get(TOPIC_URL)\n",
    "        \n",
    "        # Wait for initial content\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid=\"article-preview-card\"]'))\n",
    "        )\n",
    "        \n",
    "        # Process initial batch of posts\n",
    "        reason = process_new_posts(driver, processed_links, all_posts)\n",
    "        if reason == \"old_post_found\":\n",
    "            stop_reason = \"Stopped due to old post in initial batch\"\n",
    "            return all_posts, stop_reason\n",
    "        \n",
    "        # Scroll and process incrementally\n",
    "        while True:\n",
    "            print(\"Scrolling to bottom...\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2.5)  # Allow content to load\n",
    "            \n",
    "            # Check for end of content message\n",
    "            end_messages = driver.find_elements(By.XPATH, \"//*[contains(., 'No more posts to load')]\")\n",
    "            if end_messages:\n",
    "                stop_reason = \"Detected end of content message\"\n",
    "                print(stop_reason)\n",
    "                break\n",
    "            \n",
    "            # Process new posts\n",
    "            reason = process_new_posts(driver, processed_links, all_posts)\n",
    "            print(f\"Total posts scraped until now: {len(all_posts)}\")\n",
    "            if reason == \"old_post_found\":\n",
    "                stop_reason = \"Stopped due to old post found\"\n",
    "                print(stop_reason)\n",
    "                break\n",
    "            elif reason == \"no_new_posts\":\n",
    "                consecutive_no_new += 1\n",
    "                print(f\"No new posts detected ({consecutive_no_new}/{max_consecutive_no_new})\")\n",
    "                \n",
    "                # Break if we've had too many consecutive scrolls with no new posts\n",
    "                if consecutive_no_new >= max_consecutive_no_new:\n",
    "                    stop_reason = f\"Stopped after {max_consecutive_no_new} consecutive scrolls with no new posts\"\n",
    "                    print(stop_reason)\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_no_new = 0  # Reset counter if we found new posts\n",
    "        \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_posts, stop_reason\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    posts, reason = scrape_layoffs()\n",
    "    save_results(posts)\n",
    "    print(f\"Saved {len(posts)} posts to teamblind_layoffs_posts.json\")\n",
    "    print(f\"Stopping reason: {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted JSON to JSONL format!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Read the original JSON file (array format)\n",
    "with open('teamblind_layoffs_posts_completed_scrapping.json', 'r') as json_file:\n",
    "    # Load the entire JSON array\n",
    "    posts_array = json.load(json_file)\n",
    "\n",
    "# Step 2: Convert to JSONL format (one JSON object per line)\n",
    "with open('teamblind_layoffs_posts_completed_scrapping.jsonl', 'w') as jsonl_file:\n",
    "    for post in posts_array:\n",
    "        # Write each JSON object on its own line\n",
    "        json.dump(post, jsonl_file)\n",
    "        jsonl_file.write('\\n')  # Add newline separator\n",
    "\n",
    "print(\"Successfully converted JSON to JSONL format!\")\n",
    "\n",
    "# # Step 3: Read the JSONL file and extract URLs\n",
    "# print(\"\\nExtracting URLs from JSONL file:\")\n",
    "# with open('posts.jsonl', 'r') as jsonl_file:\n",
    "#     for line in jsonl_file:\n",
    "#         # Parse each line as JSON\n",
    "#         post = json.loads(line)\n",
    "        \n",
    "#         # Extract and print the URL\n",
    "#         url = post['url']\n",
    "#         print(f\"- {url}\")\n",
    "        \n",
    "#         # Add your processing logic here\n",
    "#         # (e.g., web scraping, data analysis, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
